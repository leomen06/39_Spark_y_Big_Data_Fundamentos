{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6cf509",
   "metadata": {},
   "source": [
    "### <a name=\"index\"></a> Index\n",
    "\n",
    "### [Introducción a Apache Spark](#mark_01)\n",
    "\n",
    "### [Introducción RDD y DataFrames.](#mark_02)\n",
    "\n",
    "### [SparkContext - Spark Session](#mark_03)\n",
    "\n",
    "### [Leer CSV a un DataFrame con SparkSession.](#mark_03.1)\n",
    "\n",
    "### [RDD Transformaciones y Acciones](#mark_04)\n",
    "\n",
    "### [RDD Acciones de modificación](#mark_05)\n",
    "\n",
    "### [ Por Qué No Usar \"collect()\"](#mark_06)\n",
    "\n",
    "### [Acciones de conteo sobre 2 o más RDDs](#mark_07)\n",
    "\n",
    "### [Operaciones numéricas](#mark_08)\n",
    "\n",
    "### [DataFrames](#mark_09)\n",
    "\n",
    "### [Inferencia tipos de datos](#mark_10)\n",
    "\n",
    "### [Operaciones sobre DF](#mark_11)\n",
    "\n",
    "   - ### [printSchema()](#mark_12)\n",
    "\n",
    "   - ### [withColumnRenamed(\"current_name\", \"new_name\").drop(\"column_name\")](#mark_13)\n",
    "\n",
    "   - ### [select(), col(\"column_name\").alias(\"new_name\")](#mark_14)\n",
    "\n",
    "   - ### [sort(\"column_name\")](#mark_15)\n",
    "\n",
    "   - ### [filter(df.column_name logic_condition)](#mark_16)\n",
    "\n",
    "### [Agrupaciones y operaciones join, multiple joins, select() sobre DF](#mark_17)\n",
    "\n",
    "### [Funciones de agrupación](#mark_18)\n",
    "\n",
    "   - ### [groupBy()](#mark_19)\n",
    "\n",
    "   - ### [ejemplo con withColumn(\"nombre_columna\", \"operacion_con_cast\")](#mark_20)\n",
    "\n",
    "   - ### [.agg() la forma correcta de hacer agregaciones](#mark_21)\n",
    "\n",
    "### [SQL](#mark_22)\n",
    "\n",
    "### [UDF User-defined function](#mark_23)\n",
    "\n",
    "### [Comprendiendo la persistencia y particionado](#mark_24)\n",
    "\n",
    "### [Preguntamos si un DF está en Cache](#mark_25)\n",
    "\n",
    "### [Que tipo de persistencia tiene un DF? \"getStorageLevel()\"](#mark_26)\n",
    "\n",
    "### [unpersist()](#mark_27)\n",
    "\n",
    "### [Cambiando el tipo de \"StorageLevel\"](#mark_28)\n",
    "\n",
    "### [Creando mi propio \"StorageLevel\", persistencia de datos](#mark_29)\n",
    "\n",
    "### [Particionando datos, RDD o DF.](#mark_30)\n",
    "\n",
    "### [Ver cantidad de particiones getNumPartitions()](#mark_31)\n",
    "\n",
    "### [Guardando los rdd o df con saveAsTextFile()](#mark_32)\n",
    "\n",
    "### [Reconstruyendo rdd desde las particiones wholeTextFiles()](#mark_33)\n",
    "\n",
    "### [Reconstruyendo rdd desde las particiones textFile() en un solo paso](#mark_34)\n",
    "\n",
    "### [Data Masking](#mark_35)\n",
    "\n",
    "### [SHA-256 (Secure Hash Algorithm 256-bit)](#mark_36)\n",
    "\n",
    "### [](#mark_3)\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf90367",
   "metadata": {},
   "source": [
    "### <a name=\"mark_01\"></a> Introducción a Apache Spark.\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_01.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_02.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "Apache Spark es un framework de trabajo para el desarrollo de grandes datos o big data y se preocupa de la velocidad y continuidad del procesamiento de datos, en contraparte de Hadoop que se preocupa por un almacenamiento grande de datos.\n",
    "\n",
    "Podemos utilizar multiples lenguajes\n",
    "\n",
    "- Java\n",
    "- Scala (Spark corre nativamente aquí)\n",
    "- Python\n",
    "- R\n",
    "¿Que nos es Apache Spark? No es una base de datos\n",
    "\n",
    "**OLAP (Online analytical processing)**: Es un sistema de recuperación de datos y análisis de datos en linea.\n",
    "\n",
    "**OLTP (Online Transaction Processing)**: Es un sistema transaccional en línea y gestiona la modificación de la base de datos.\n",
    "\n",
    "Spark debe estar conectado a un Data warehouse para poder aprovechar toda su funcionalidad.\n",
    "\n",
    "![](img_03.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_04.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_05.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c819bc9",
   "metadata": {},
   "source": [
    "### <a name=\"mark_02\"></a> Introducción RDD y DataFrames.\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_06.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_07.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "- Características de los RDD:\n",
    "\n",
    "Principal abstracción de datos: Es la unidad básica, existen desde su inicio hasta su versión 3.0.\n",
    "\n",
    "Distribución: Los RDD se dritribuyen y particionan a lo largo del clúster.\n",
    "\n",
    "Creación simple: Al no poseer estructura formalmente, adoptan las más intuitiva.\n",
    "\n",
    "Inmutabilidad: Posterior a su creación no se pueden modificar\n",
    "\n",
    "Ejecución perezosa: A menos que se realice una acción, todo lo que se escribió de código, no corre.\n",
    "\n",
    "Por ejemplo, si estoy cargando un archivo que no existe no me voy a dar cuenta, también puede pasar que si el archivo existe pero posee error tampoco me voy a dar cuenta hasta que todo el archivo sea cargado.\n",
    "\n",
    "- Aquí nacen dos diferencias Transformaciones y Acciones:\n",
    "\n",
    "![](img_09.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "- Todas las operaciones que sean \"Transformaciones\" las podrémos escribir sin problema.\n",
    "- Una vez que realizo una \"acción\" estoy dandole vida a lo que hago, con lo cual hay que ser cuidadoso con las operaciones que realicemos.\n",
    "\n",
    "- Para el siguiente ejemplo:\n",
    "![](img_10.png)\n",
    "\n",
    "1. Cargamos un texto.\n",
    "2. Genero 2 nuevos RDD a partir del RDD existente, uno tendrá todas las palabras \"Comala\" y el otro las palabras \"Páramo\"\n",
    "3. Luego realizamos una intersección de ambos.\n",
    "- Hasta este momento el archivo que estoy cargando pdría no existir, incluso tener algunos errores en las filtros, \"pero\" hasta que no ejecute la \"Acción COUNT\" no nos daremos cuenta que tenemos un error de fondo, y debemos tener cuidado al momento de trabajar con los RDD.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_08.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "- Algunos problemas a lo largo de la vida de nuestro ETL:\n",
    "\n",
    "Muchos RDD conviviendo\n",
    "\n",
    "RDD que ya son basura y tenemos que hacer acciones para que Garbage Collector se ejecute.\n",
    "\n",
    "- Características de los DataFrame:\n",
    "\n",
    "Son una capa superior que existe sobre los RDD.\n",
    "\n",
    "Poseen estructura (una columna).\n",
    "\n",
    "![](img_11.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_12.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "Formato: A diferencia de un RDD poseen columnas, las cuales pueden ser string, double, int, float, date, etc.\n",
    "\n",
    "Optimización: Poseen una mejor implementación, lo cual los hace preferibles.\n",
    "\n",
    "Facilidad de creación: Se pueden crear desde una base de datos externa, archivo o RDD existente.\n",
    "\n",
    "- ¿Cuándo usar un RDD?\n",
    "\n",
    "![](img_13.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "Cuando te interesa controlar el flujo de Spark, para saltearte algunas cosas que sabes que Spark no está optimizando bien.\n",
    "\n",
    "Si eres usuario Python, convertir a RDD un conjunto permite mejor control de datos.\n",
    "\n",
    "Estás conectándote a versiones antiguas de spark.\n",
    "\n",
    "- ¿Cuándo usar DataFrames?\n",
    "\n",
    "![](img_14.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "Si poseemos semánticas de datos complicadas.\n",
    "\n",
    "Vamos a realizar tareas de alto nivel como filtros, mapeos, agregaciones, promedios o sumas.\n",
    "\n",
    "Si vamos a usar sentencias SQL.\n",
    "\n",
    "### Todas las aplicaciones en Spark poseen un manejador central de programa (Driver) y varios ejecutores que se crean a lo largo del clúster, estas son las computadoras que realizarán las tareas en paralelo y finalmente devolverán los valores al driver, la aplicación central.\n",
    "\n",
    "Para nuestros fines, debido a que se usa un modelo stand-alone, solo se contará con un driver y un ejecutor, ambos alojados en la misma computadora.\n",
    "\n",
    "- RDD\n",
    "\n",
    "Para poder realizar estas tareas, Spark posee desde su versión 1.0 los RDD (Resilient Distributed Dataset), los cuales son tolerantes a fallos y pueden ser distribuidos a lo largo de los nodos del clúster.\n",
    "\n",
    "Los RDD pueden ser creados al cargar datos de manera distribuida, como es desde un HDFS, Cassanda, Hbase o cualquier sistema de datos soportado por Hadoop, pero también por colecciones de datos de Scala o Python, además de poder ser leídos desde archivos en el sistema local.\n",
    "\n",
    "En visión general, un RDD puede ser visto como un set de datos los cuales soportan solo dos tipos de operaciones: **transformaciones y acciones**.\n",
    "\n",
    "Las transformaciones permiten crear un nuevo RDD a partir de uno previamente existente, mientras que las acciones retornan un valor al driver de la aplicación. El núcleo de operación del paradigma de Spark es la ejecución perezosa (Lazy), es decir que las transformaciones solo serán calculadas posterior a una llamada de acción.\n",
    "\n",
    "Además, los RDD poseen una familiaridad con el paradigma orientado a objetos, lo cual permite que podamos realizar operaciones de bajo nivel a modo. **Map, filter y reduce** son tres de las operaciones más comunes.\n",
    "\n",
    "Una de las grandes ventajas que ofrecen los RDD es la compilación segura; por su particularidad de ejecución perezosa, se calcula si se generará un error o no antes de ejecutarse, lo cual permite identificar problemas antes de lanzar la aplicación. \n",
    "\n",
    "El peor que podemos encontrar con los RDD es que no son correctamente tratados por el Garbage collector y cuando las lógicas de operación se hacen complejas, su uso puede resultar poco práctico, aquí entran los DataFrames.\n",
    "\n",
    "- DataFrames\n",
    "\n",
    "Esos componentes fueron agregados en la versión 1.3 de Spark y pueden ser invocados con el contexto espacial de Spark SQL. Como indica su nombre, es un módulo especialmente desarrollado para ser ejecutado con instrucciones parecidas al SQL estándar.\n",
    "\n",
    "De la misma forma, como los RDD, estos pueden ser creados a partir de archivos, tablas tipo Hive, bases de datos externas y RDD o DataFrames existentes.\n",
    "\n",
    "El primer detalle que salta cuando creamos un DataFrame es que poseen columnas nombradas, lo que a nivel conceptual es como trabajar con un DataFrame de Pandas. Con la excepción que a nivel interno Spark trabaja con Scala, lo cual le asigna a cada columna el tipo de dato Row, un tipo especial de objeto sin tipo definido.\n",
    "\n",
    "Pero no es todo, los DataFrames implementan un sistema llamado Catalyst, el cual es un motor de optimización de planes de ejecución, parecido al que usan las bases de datos, pero diseñado para la cantidad de datos propia de Spark, aunado a eso, se tiene implementado un optimizador de memoria y consumo de CPU llamado Tungsten, el cual determina cómo se convertirán los planes lógicos creados por Catalyst a un plan físico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f822c7b",
   "metadata": {},
   "source": [
    "### <a name=\"mark_03\"></a> SparkContext - SparkSession.\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa15fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable # ver --> \"solucion python worker versiones\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable # ver --> \"solucion python worker versiones\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2efd41",
   "metadata": {},
   "source": [
    "\n",
    "### Ver --> [solucion python worker versiones](#solucion_python_worker_versiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10c8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70062d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a17acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando la primer sesión\n",
    "'''spark = SparkSession.builder \\\n",
    "    .master(\"local\")\\ #Donde va estar viviendo mi sesión\n",
    "    .appName(\"mi_primer_sesion\")\\ #Nombre de la sesión.\n",
    "    .getOrCreate() #creación'''\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"mi_primer_sesion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2a951",
   "metadata": {},
   "source": [
    "### Differencias entre \"context\" y \"session\".\n",
    "\n",
    "basicamente es asi.\n",
    "SparkContext = versiones 1 y 2 de Spark, funciones, metodos para interractuar con Spark\n",
    "\n",
    "```py\n",
    "sc = SparkContext(master=\"local\", appName=\"mi_primer_contexto\")\n",
    "```\n",
    "\n",
    "SparkSession = SparkContext + nuevas funciones, mejores formas de configuraciones que no están presentes en las versiones anteriores de \"SparkContext\"\n",
    "- Desde SparkSession podemos invocar Contextos y convertirlos en Sesiones (esto es útil si estamos utilizando versiones antigüas, con lo cual no hay que reconstruir nada).\n",
    "\n",
    "### Pasar de SparkContext --> SparkSession:\n",
    "```py\n",
    "sc_a_ss = SparkSession(sc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcddacdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.145:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mi_primer_sesion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f14d94b7c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark #Spark cuenta con un UI, monitor gráfico \"Spark UI\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc294aae",
   "metadata": {},
   "source": [
    "### Spark UI.\n",
    "\n",
    "- Jobs --> De tener DF o RDD ejecutados podemos ver los momentos de despliegue.\n",
    "- Stages --> El paso a paso de los DF o RDD cargados.\n",
    "- Environments --> checkeo del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6997f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al finalizar la sesión debe ser cerrada para que no siga consumiendo recursos.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b814920",
   "metadata": {},
   "source": [
    "### <a name=\"mark_03.1\"></a> Leer CSV a un DataFrame con SparkSession.\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder\\\n",
    "    .master('local')\\\n",
    "    .appName('test_session_01')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62297b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_path='./curso_spark_git_clone/curso-apache-spark-platzi/files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "deporte_df=spark.read.options(inferSchema=True, header=True, delimiter=',')\\\n",
    "    .csv(general_path + 'deporte.csv')\n",
    "\n",
    "deportista_df=spark.read.options(inferSchema=True, header=True, delimiter=',')\\\n",
    "    .csv(general_path+'deportista.csv')\n",
    "\n",
    "deportista2_df=spark.read.options(inferSchema=True, header=False, delimiter=',')\\\n",
    "    .csv(general_path+'deportista2.csv')\n",
    "\n",
    "evento_df=spark.read.options(inferSchema=True, header=True, delimiter=',')\\\n",
    "    .csv(general_path+'evento.csv')\n",
    "\n",
    "juegos_df=spark.read.options(inferSchema=True, header=True, delimiter=',')\\\n",
    "    .csv(general_path+'juegos.csv')\n",
    "\n",
    "paises_df=spark.read.options(inferSchema=True, header=True, delimiter=',')\\\n",
    "    .csv(general_path+'paises.csv')\n",
    "\n",
    "resultados_df=spark.read.options(inferSchema=True, header=True, delimiter=',')\\\n",
    "    .csv(general_path+'resultados.csv')\n",
    "\n",
    "print('deporte_df')\n",
    "deporte_df.printSchema()\n",
    "print('deportista_df')\n",
    "deportista_df.printSchema()\n",
    "print('deportista2_df')\n",
    "deportista2_df.printSchema()\n",
    "print('evento_df')\n",
    "evento_df.printSchema()\n",
    "print('juegos_df')\n",
    "juegos_df.printSchema()\n",
    "print('paises_df')\n",
    "paises_df.printSchema()\n",
    "print('resultados_df')\n",
    "resultados_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df3a82",
   "metadata": {},
   "source": [
    "### <a name=\"mark_04\"></a> RDD Transformaciones y Acciones.\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b181cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53928b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_01 = SparkContext(master=\"local\", appName=\"transformaciones_y_acciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baee04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#los rdd son distribuidos y paralelos, y podemos crearlos de la siguiente forma\n",
    "rdd_01=sc_01.parallelize([1,2,3]) #datos distribuidos en el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb3a0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualización\n",
    "rdd_01.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76975277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.145:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>transformaciones_y_acciones</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=transformaciones_y_acciones>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882342c",
   "metadata": {},
   "source": [
    "### Primer Job realizados\n",
    "\n",
    "![](img_15.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "- Inspeccionando el trabajo.\n",
    "\n",
    "![](img_16.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6609603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bed09df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14520\r\n",
      "drwxr-xr-x 2 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  9 09:38 .\r\n",
      "drwxr-xr-x 5 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  7 13:11 ..\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01     946 Oct  7 13:11 deporte.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2764536 Oct  7 13:11 deportista.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2807091 Oct  7 13:11 deportista2.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2776782 Oct  7 13:11 deportistaError.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   33262 Oct  7 13:11 evento.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    1978 Oct  7 13:11 juegos.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01  255853 Oct  7 13:11 modelo_relacional.jpg\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   23606 Oct  7 13:11 paises.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 6172796 Oct  7 13:11 resultados.csv\r\n"
     ]
    }
   ],
   "source": [
    "#Visualizando los archivos clonados desde GitHub.\n",
    "!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a73b9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./curso_spark_git_clone/curso-apache-spark-platzi/files/\"\n",
    "\n",
    "#creand el rdd equipos_olimplicos_rdd\n",
    "equipos_olimpicos_rdd=sc_01.textFile(path+\"paises.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d5318f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['id', 'equipo', 'sigla'],\n",
       " ['1', '30. Februar', 'AUT'],\n",
       " ['2', 'A North American Team', 'MEX'],\n",
       " ['3', 'Acipactli', 'MEX'],\n",
       " ['4', 'Acturus', 'ARG']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tomamos las primeras 5 líneas.\n",
    "equipos_olimpicos_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9f9877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_01.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e81b202",
   "metadata": {},
   "source": [
    "### <a name=\"mark_05\"></a> RDD Acciones de modificación.\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f1b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c4756a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_02=SparkContext(master=\"local\", appName=\"acciones_de_modificacion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "531ab8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./curso_spark_git_clone/curso-apache-spark-platzi/files/\"\n",
    "\n",
    "#creand el rdd equipos_olimplicos_rdd\n",
    "equipos_olimpicos_rdd_02=sc_02.textFile(path+\"paises.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8080ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'equipo', 'sigla'],\n",
       " ['1', '30. Februar', 'AUT'],\n",
       " ['2', 'A North American Team', 'MEX'],\n",
       " ['3', 'Acipactli', 'MEX'],\n",
       " ['4', 'Acturus', 'ARG'],\n",
       " ['5', 'Afghanistan', 'AFG'],\n",
       " ['6', 'Akatonbo', 'IRL'],\n",
       " ['7', 'Alain IV', 'SUI'],\n",
       " ['8', 'Albania', 'ALB'],\n",
       " ['9', 'Alcaid', 'POR'],\n",
       " ['10', 'Alcyon-6', 'FRA'],\n",
       " ['11', 'Alcyon-7', 'FRA'],\n",
       " ['12', 'Aldebaran', 'ITA'],\n",
       " ['13', 'Aldebaran II', 'ITA'],\n",
       " ['14', 'Aletta', 'IRL']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_olimpicos_rdd_02.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af2baf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de los países, tomo el index 2\n",
    "lista_paises=equipos_olimpicos_rdd_02.map(lambda x: x[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5785e9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sigla',\n",
       " 'AUT',\n",
       " 'MEX',\n",
       " 'MEX',\n",
       " 'ARG',\n",
       " 'AFG',\n",
       " 'IRL',\n",
       " 'SUI',\n",
       " 'ALB',\n",
       " 'POR',\n",
       " 'FRA',\n",
       " 'FRA',\n",
       " 'ITA',\n",
       " 'ITA',\n",
       " 'IRL']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_paises.take(15)#como vemos acá hay duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03be0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quitando los duplicados\n",
    "lista_paises_distinct=lista_paises.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bb47eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sigla',\n",
       " 'AUT',\n",
       " 'MEX',\n",
       " 'ARG',\n",
       " 'AFG',\n",
       " 'IRL',\n",
       " 'SUI',\n",
       " 'ALB',\n",
       " 'POR',\n",
       " 'FRA',\n",
       " 'ITA',\n",
       " 'ALG',\n",
       " 'SWE',\n",
       " 'URS',\n",
       " 'AUS']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_paises_distinct.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9747a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contando la cantidad de países\n",
    "lista_paises_count=lista_paises_distinct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fbdfb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad total de paises: 231\n"
     ]
    }
   ],
   "source": [
    "print(f\"cantidad total de paises: {lista_paises_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08b99773",
   "metadata": {},
   "outputs": [],
   "source": [
    "equipos_por_pais=equipos_olimpicos_rdd_02.map(lambda x: (x[2],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58501ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sigla', 'equipo'),\n",
       " ('AUT', '30. Februar'),\n",
       " ('MEX', 'A North American Team'),\n",
       " ('MEX', 'Acipactli'),\n",
       " ('ARG', 'Acturus'),\n",
       " ('AFG', 'Afghanistan'),\n",
       " ('IRL', 'Akatonbo'),\n",
       " ('SUI', 'Alain IV'),\n",
       " ('ALB', 'Albania'),\n",
       " ('POR', 'Alcaid'),\n",
       " ('FRA', 'Alcyon-6'),\n",
       " ('FRA', 'Alcyon-7'),\n",
       " ('ITA', 'Aldebaran'),\n",
       " ('ITA', 'Aldebaran II'),\n",
       " ('IRL', 'Aletta')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_por_pais.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df783766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[38] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_por_pais=equipos_olimpicos_rdd_02.map(lambda x: (x[2],x[1]))\n",
    "equipos_por_pais.groupByKey()\n",
    "\n",
    "#observar que se agruparon por \"sigla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1986e5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sigla', 'equipo'),\n",
       " ('AUT', '30. Februar'),\n",
       " ('MEX', 'A North American Team'),\n",
       " ('MEX', 'Acipactli'),\n",
       " ('ARG', 'Acturus'),\n",
       " ('AFG', 'Afghanistan'),\n",
       " ('IRL', 'Akatonbo'),\n",
       " ('SUI', 'Alain IV'),\n",
       " ('ALB', 'Albania'),\n",
       " ('POR', 'Alcaid'),\n",
       " ('FRA', 'Alcyon-6'),\n",
       " ('FRA', 'Alcyon-7'),\n",
       " ('ITA', 'Aldebaran'),\n",
       " ('ITA', 'Aldebaran II'),\n",
       " ('IRL', 'Aletta')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_por_pais.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e00b85c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en este caso \"mapValues()\" genera una lista con un país y su lista de equipos, luego\\n\"len\" nos dá la longitud de esa lista, o sea la cantidad de equipos por país'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_por_pais=equipos_olimpicos_rdd_02\\\n",
    "    .map(lambda x: (x[2],x[1]))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(len)\n",
    "'''en este caso \"mapValues()\" genera una lista con un país y su lista de equipos, luego\n",
    "\"len\" nos dá la longitud de esa lista, o sea la cantidad de equipos por país'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c16f572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sigla', 1),\n",
       " ('AUT', 11),\n",
       " ('MEX', 9),\n",
       " ('ARG', 18),\n",
       " ('AFG', 1),\n",
       " ('IRL', 7),\n",
       " ('SUI', 17),\n",
       " ('ALB', 1),\n",
       " ('POR', 21),\n",
       " ('FRA', 155),\n",
       " ('ITA', 36),\n",
       " ('ALG', 1),\n",
       " ('SWE', 52),\n",
       " ('URS', 16),\n",
       " ('AUS', 23)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_por_pais.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0eef2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "equipos_por_pais=equipos_olimpicos_rdd_02\\\n",
    "    .map(lambda x: (x[2],x[1]))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bef7a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sigla', ['equipo']),\n",
       " ('AUT',\n",
       "  ['30. Februar',\n",
       "   'Austria',\n",
       "   'Austria-1',\n",
       "   'Austria-2',\n",
       "   'Breslau',\n",
       "   'Brigantia',\n",
       "   'Donar III',\n",
       "   'Evita VI',\n",
       "   'May-Be 1960',\n",
       "   '\"R.-V. Germania; Leitmeritz\"',\n",
       "   'Surprise']),\n",
       " ('MEX',\n",
       "  ['A North American Team',\n",
       "   'Acipactli',\n",
       "   'Chamukina',\n",
       "   'Mexico',\n",
       "   'Mexico-1',\n",
       "   'Mexico-2',\n",
       "   'Nausikaa 4',\n",
       "   'Tlaloc',\n",
       "   'Xolotl']),\n",
       " ('ARG',\n",
       "  ['Acturus',\n",
       "   'Antares',\n",
       "   'Arcturus',\n",
       "   'Ardilla',\n",
       "   'Argentina',\n",
       "   'Argentina-1',\n",
       "   'Argentina-2',\n",
       "   'Blue Red',\n",
       "   'Covunco III',\n",
       "   'Cupidon III',\n",
       "   'Djinn',\n",
       "   'Gullvinge',\n",
       "   'Matrero II',\n",
       "   'Mizar',\n",
       "   'Pampero',\n",
       "   'Rampage',\n",
       "   'Tango',\n",
       "   'Wiking']),\n",
       " ('AFG', ['Afghanistan']),\n",
       " ('IRL',\n",
       "  ['Akatonbo',\n",
       "   'Aletta',\n",
       "   'Ireland',\n",
       "   'Ireland-1',\n",
       "   'Ireland-2',\n",
       "   'The Cloud',\n",
       "   'Three Leaves']),\n",
       " ('SUI',\n",
       "  ['Alain IV',\n",
       "   'Ali-Baba IV',\n",
       "   'Ali-Baba IX',\n",
       "   'Ali-Baba VI',\n",
       "   'Baccara',\n",
       "   'Ballerina IV',\n",
       "   'Fantasio III',\n",
       "   'Kln',\n",
       "   'Lerina',\n",
       "   'Pousse-Moi Pas VII',\n",
       "   'Switzerland',\n",
       "   'Switzerland-1',\n",
       "   'Switzerland-2',\n",
       "   'Tim-Tam III',\n",
       "   'Ylliam II',\n",
       "   'Ylliam VII',\n",
       "   'Ylliam VIII']),\n",
       " ('ALB', ['Albania']),\n",
       " ('POR',\n",
       "  ['Alcaid',\n",
       "   'Argus',\n",
       "   'Calcinhas',\n",
       "   'Camelia',\n",
       "   'Ciocca III',\n",
       "   'Espadarte',\n",
       "   'Espardate',\n",
       "   'Faneca',\n",
       "   'Galopin De La Font',\n",
       "   'Grifo III',\n",
       "   'Grifo IV',\n",
       "   'Hannover',\n",
       "   \"Ma'Lindo\",\n",
       "   'Notavel',\n",
       "   'Oxalis',\n",
       "   'Portugal',\n",
       "   'Portugal-1',\n",
       "   'Portugal-2',\n",
       "   'Sjhxa',\n",
       "   'Symphony',\n",
       "   'Vicking']),\n",
       " ('FRA',\n",
       "  ['Alcyon-6',\n",
       "   'Alcyon-7',\n",
       "   'Allegro',\n",
       "   'Amulet-3',\n",
       "   'Amulet-7',\n",
       "   'Ariette-10',\n",
       "   'Ariette-8',\n",
       "   'Astrid III',\n",
       "   'Baby-1',\n",
       "   'Baby-9',\n",
       "   '\"Bagatelle Polo Club; Paris\"',\n",
       "   'C.V.A.-14',\n",
       "   'C.V.A.-7',\n",
       "   'Calimucho',\n",
       "   'Calypse II',\n",
       "   'Camille',\n",
       "   'Carabinier-15',\n",
       "   'Carabinier-5',\n",
       "   \"Cercle de l'Aviron Roubaix-4\",\n",
       "   'Cercle Nautique de Reims-4',\n",
       "   'Cinara-13',\n",
       "   'Club Nautique de Dieppe-5',\n",
       "   'Club Nautique de Franais-1',\n",
       "   'Club Nautique de Lyon-2',\n",
       "   'Colette-10',\n",
       "   'Colette-12',\n",
       "   'Compigne Polo Club',\n",
       "   'Crabe I-11',\n",
       "   'Crabe I-2',\n",
       "   'Crabe I-3',\n",
       "   'Crabe II-1',\n",
       "   'Crabe II-12',\n",
       "   'Crabe II-4',\n",
       "   'Crocodile-11',\n",
       "   'Crocodile-13',\n",
       "   'Cupidon Viking',\n",
       "   'Damoiselle',\n",
       "   'Demi-Mondaine-15',\n",
       "   'Demi-Mondaine-17',\n",
       "   'Diabolo St Maurice',\n",
       "   'Dick-8',\n",
       "   'Ducky-16',\n",
       "   'Ducky-4',\n",
       "   'EA II',\n",
       "   'Eissero VI',\n",
       "   'Esterel-1',\n",
       "   'Fada',\n",
       "   'Fantlet-2',\n",
       "   'Fantlet-7',\n",
       "   'Favorite-1',\n",
       "   'Favorite-17',\n",
       "   'Femur-1',\n",
       "   'Femur-18',\n",
       "   'France',\n",
       "   'France-1',\n",
       "   'France-2',\n",
       "   'France-3',\n",
       "   'France-4',\n",
       "   'Freia-19',\n",
       "   'Freia-5',\n",
       "   'Frip IV',\n",
       "   'Galopin-20',\n",
       "   'Galopin-9',\n",
       "   'Gam II',\n",
       "   'Gilliatt V',\n",
       "   'Giselle-6',\n",
       "   'Gitana-2',\n",
       "   'Gitana-21',\n",
       "   'Guyoni',\n",
       "   'Gwendoline-2',\n",
       "   'Gwendoline-22',\n",
       "   'Gyp-23',\n",
       "   'Gyp-6',\n",
       "   'Hb-20',\n",
       "   'Hb-24',\n",
       "   'Jeanette-25',\n",
       "   'Jeannette-18',\n",
       "   'Kannibaltje',\n",
       "   \"L'Aile VI\",\n",
       "   'LaBandera',\n",
       "   'Leipzig',\n",
       "   'Libellule de Paris',\n",
       "   'Libellule de Paris-3',\n",
       "   'Mac Miche',\n",
       "   'Mamie-4',\n",
       "   'Mamie-6',\n",
       "   'Marsouin-26',\n",
       "   'Marsouin-8',\n",
       "   'Martha-1',\n",
       "   'Martha-27',\n",
       "   'Mascaret-28',\n",
       "   'Mascaret-4',\n",
       "   'Mignon-29',\n",
       "   'Mignon-3',\n",
       "   'Namoussa',\n",
       "   'Nina Claire-2',\n",
       "   'Nina Claire-30',\n",
       "   'Olle',\n",
       "   'Pettit-Poucet-6',\n",
       "   'Pierre et Jean-3',\n",
       "   'Pierre et Jean-4',\n",
       "   'Pigoule',\n",
       "   'Pirouette-31',\n",
       "   'Pirouette-5',\n",
       "   'Plume-patte-32',\n",
       "   'Plume-Patte-5',\n",
       "   'Pupilles de Neptune de Lille #1-3',\n",
       "   'Pupilles de Neptune de Lille #2-1',\n",
       "   'Pupilles de Neptune de Lille-1',\n",
       "   \"Qu'Importe\",\n",
       "   'Quand-Mme-2',\n",
       "   'Quand-Mme-33',\n",
       "   'Racing Club de France',\n",
       "   'Red Indian',\n",
       "   'Rose Pompon',\n",
       "   'Rowing Club Castillon-3',\n",
       "   'Rozenn-3',\n",
       "   'Sandra',\n",
       "   'Sarcelle-3',\n",
       "   'Sarcelle-35',\n",
       "   'Scamasaxe-2',\n",
       "   'Scamasaxe-3',\n",
       "   'Scamasaxe-34',\n",
       "   'Sidi Fekkar-36',\n",
       "   'Sidi-Fekkar-14',\n",
       "   'Singy-37',\n",
       "   'Singy-7',\n",
       "   'Snowten III',\n",
       "   'Societ Nautique de la Marne',\n",
       "   'Societ Nautique de la Marne-1',\n",
       "   'Societ Nautique de la Marne-3',\n",
       "   'Socit Nautique de Bayonne',\n",
       "   'Socit Nautique de Bayonne-1',\n",
       "   'Socit Nautique de Bayonne-2',\n",
       "   'Socit Nautique de la Basse Seine-1',\n",
       "   'Socit Nautique de la Basse Seine-2',\n",
       "   'Souriceau-38',\n",
       "   'Souriceau-4',\n",
       "   'Souvenance',\n",
       "   'Stade Franais AC-2',\n",
       "   'Suzon IV-39',\n",
       "   'Suzon IV-5',\n",
       "   'Tornade-16',\n",
       "   'Tournade-40',\n",
       "   'Tramontane',\n",
       "   'Tritons Lillois-2',\n",
       "   'Turquoise-1',\n",
       "   'Turquoise-3',\n",
       "   'Union des Socits Franais de Sports Athletiques',\n",
       "   'United States/France',\n",
       "   'USFSA',\n",
       "   'Verveine-19',\n",
       "   'Verveine-41',\n",
       "   'Virginie',\n",
       "   'Whitini Star']),\n",
       " ('ITA',\n",
       "  ['Aldebaran',\n",
       "   'Aldebaran II',\n",
       "   'Aretusa',\n",
       "   'Argeste',\n",
       "   'Augsburg',\n",
       "   'Ausonia',\n",
       "   'Bamba',\n",
       "   'Barion/Bari-2',\n",
       "   'Bucintoro Venezia',\n",
       "   'Bucintoro Venezia-1',\n",
       "   'Ciocca',\n",
       "   'Ciocca II',\n",
       "   'Enotria',\n",
       "   'Esperia',\n",
       "   'Galatea II',\n",
       "   'Grifone',\n",
       "   'Italia',\n",
       "   'Italy',\n",
       "   'Italy-1',\n",
       "   'Italy-2',\n",
       "   'Italy-3',\n",
       "   'Legionario',\n",
       "   '\"Marinai della nave da guerra \"\"Varese\"\"\"',\n",
       "   'Mebi',\n",
       "   'Merope',\n",
       "   'Merope III',\n",
       "   'Mirtala',\n",
       "   'Pegaso',\n",
       "   'Pistoja/Firenze',\n",
       "   'Roma',\n",
       "   'Romolo',\n",
       "   'Twins II',\n",
       "   'Twins VIII',\n",
       "   'Umberta V',\n",
       "   'Venilia',\n",
       "   'Voloira II']),\n",
       " ('ALG', ['Algeria']),\n",
       " ('SWE',\n",
       "  ['Ali-Baba II',\n",
       "   'Aloha II',\n",
       "   'Bissbi',\n",
       "   'Briar',\n",
       "   'Chance',\n",
       "   'Denmark/Sweden',\n",
       "   'Elsie',\n",
       "   'Erna Signe',\n",
       "   'Floresco',\n",
       "   'Freja',\n",
       "   'Galejan',\n",
       "   'Gteborgs Roddfrening-2',\n",
       "   'Gteborgs Roddklubb-1',\n",
       "   'Hayama',\n",
       "   'Hojwa',\n",
       "   'Humbug V',\n",
       "   'Ilderim',\n",
       "   'Ingegerd',\n",
       "   'K.S.S.S. 1912-2',\n",
       "   'Kerstin-1',\n",
       "   'Kitty-1',\n",
       "   'Kuling',\n",
       "   'Kullan',\n",
       "   'Lasha',\n",
       "   'Lotta IV',\n",
       "   'Marga-2',\n",
       "   'Mari',\n",
       "   'May Be',\n",
       "   'May Be VII',\n",
       "   'Roddklubben af 1912-1',\n",
       "   'Roddklubben af 1912-2',\n",
       "   'Rush V',\n",
       "   'Rush VII',\n",
       "   'Saga-2',\n",
       "   'Sans Atout-1',\n",
       "   'Sass-2',\n",
       "   'Sif',\n",
       "   'Sjovinge',\n",
       "   'Slaghoken',\n",
       "   'Slaghoken II',\n",
       "   'Solos Carex',\n",
       "   'Sunshine',\n",
       "   'Sweden',\n",
       "   'Sweden-1',\n",
       "   'Sweden-2',\n",
       "   'Sweden-3',\n",
       "   'Sweden-4',\n",
       "   'Swedish Star',\n",
       "   'Sylvia',\n",
       "   'Tornado',\n",
       "   'Vaxholm Roddklubb',\n",
       "   'Vinga-1']),\n",
       " ('URS',\n",
       "  ['Almaz',\n",
       "   'Burevestnik',\n",
       "   'Druzhba',\n",
       "   'Kon-Tiki',\n",
       "   'Korshun',\n",
       "   'Neptun II',\n",
       "   'Nokaut II',\n",
       "   'Olen',\n",
       "   'Persey',\n",
       "   'Soviet Union',\n",
       "   'Soviet Union-1',\n",
       "   'Soviet Union-2',\n",
       "   'Soviet Union-3',\n",
       "   'Tulilind',\n",
       "   'Uragan',\n",
       "   'Viktoriya']),\n",
       " ('AUS',\n",
       "  ['Amateur Athletic Association',\n",
       "   'Australia',\n",
       "   'Australia-1',\n",
       "   'Australia-2',\n",
       "   'Australia-3',\n",
       "   'Australia/Great Britain',\n",
       "   'Barrenjoey',\n",
       "   'Buraddoo',\n",
       "   'Cambria',\n",
       "   'Diablo',\n",
       "   'Falcon IV',\n",
       "   'Falcon VI',\n",
       "   'Gabbiano',\n",
       "   'Greenoaks Dundee',\n",
       "   'Hornet',\n",
       "   'Maryke',\n",
       "   'Moorina',\n",
       "   'Naiad',\n",
       "   'Pakaria',\n",
       "   'Paula',\n",
       "   'Quando Quando',\n",
       "   'Relampago',\n",
       "   'Vinha'])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_por_pais.take(15)# acá se ve la lista de equipos por paises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae951f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "equipos_arg=equipos_olimpicos_rdd_02.filter(lambda line: \"ARG\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b910754d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4', 'Acturus', 'ARG'],\n",
       " ['37', 'Antares', 'ARG'],\n",
       " ['42', 'Arcturus', 'ARG'],\n",
       " ['43', 'Ardilla', 'ARG'],\n",
       " ['45', 'Argentina', 'ARG'],\n",
       " ['46', 'Argentina-1', 'ARG'],\n",
       " ['47', 'Argentina-2', 'ARG'],\n",
       " ['119', 'Blue Red', 'ARG'],\n",
       " ['238', 'Covunco III', 'ARG'],\n",
       " ['252', 'Cupidon III', 'ARG'],\n",
       " ['288', 'Djinn', 'ARG'],\n",
       " ['436', 'Gullvinge', 'ARG'],\n",
       " ['644', 'Matrero II', 'ARG'],\n",
       " ['672', 'Mizar', 'ARG'],\n",
       " ['774', 'Pampero', 'ARG'],\n",
       " ['843', 'Rampage', 'ARG'],\n",
       " ['1031', 'Tango', 'ARG'],\n",
       " ['1162', 'Wiking', 'ARG']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equipos_arg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36aa8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_02.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390f39c",
   "metadata": {},
   "source": [
    "### <a name=\"mark_06\"></a> Por Qué No Usar \"collect()\".\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- \"collect()\" toma todos los RDD distribuidos, que contengan los datos que se buscan, son enviados a la computadora que ejecutó el \"collect()\", esto en Big Data va a generar problemas al momento de intentar procesar grandes cantidades de datos.\n",
    "\n",
    "- Una buena alternativa para poder contar la cantidad de datos que tenemos es usar un countApprox(milisegundos), esta instrucción solo cuenta hasta 20 miliseg de proceso y arroja el valor.\n",
    "```python\n",
    "cant_aprox_datos = equipos_olimpicos_rdd_02.countApprox(20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca624f8",
   "metadata": {},
   "source": [
    "### <a name=\"mark_07\"></a> Acciones de conteo sobre 2 o más RDDs.\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28bb950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14520\r\n",
      "drwxr-xr-x 2 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  9 09:38 .\r\n",
      "drwxr-xr-x 5 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  7 13:11 ..\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01     946 Oct  7 13:11 deporte.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2764536 Oct  7 13:11 deportista.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2807091 Oct  7 13:11 deportista2.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2776782 Oct  7 13:11 deportistaError.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   33262 Oct  7 13:11 evento.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    1978 Oct  7 13:11 juegos.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01  255853 Oct  7 13:11 modelo_relacional.jpg\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   23606 Oct  7 13:11 paises.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 6172796 Oct  7 13:11 resultados.csv\r\n"
     ]
    }
   ],
   "source": [
    "#visualizando los RDDs que tenemos\n",
    "!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"deportista.csv\" contiene el encabezado, y \"deportista2.csv\" sin encabezado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1cadee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60c2968c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=new_rdd, master=local) created by __init__ at /tmp/ipykernel_489/6268609.py:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc_03\u001b[38;5;241m=\u001b[39mSparkContext(master\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, appName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macciones_en_2_rdd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/pyspark/context.py:445\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    450\u001b[0m             currentAppName,\n\u001b[1;32m    451\u001b[0m             currentMaster,\n\u001b[1;32m    452\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    453\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    454\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    455\u001b[0m         )\n\u001b[1;32m    456\u001b[0m     )\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=new_rdd, master=local) created by __init__ at /tmp/ipykernel_489/6268609.py:1 "
     ]
    }
   ],
   "source": [
    "sc_03=SparkContext(master=\"local\", appName=\"acciones_en_2_rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5f98df7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc_03' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./curso_spark_git_clone/curso-apache-spark-platzi/files/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m deportista_encabezado\u001b[38;5;241m=\u001b[39msc_03\u001b[38;5;241m.\u001b[39mtextFile(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeportista.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m deportista2_datos\u001b[38;5;241m=\u001b[39msc_03\u001b[38;5;241m.\u001b[39mtextFile(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeportista2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m paises\u001b[38;5;241m=\u001b[39msc_03\u001b[38;5;241m.\u001b[39mtextFile(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaises.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc_03' is not defined"
     ]
    }
   ],
   "source": [
    "path=\"./curso_spark_git_clone/curso-apache-spark-platzi/files/\"\n",
    "deportista_encabezado=sc_03.textFile(path+\"deportista.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "deportista2_datos=sc_03.textFile(path+\"deportista2.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "paises=sc_03.textFile(path+\"paises.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "resultados=sc_03.textFile(path+\"resultados.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3226528",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportis_union=deportista_encabezado.union(deportista2_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16e97a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad en deportista.csv: 67787\n",
      "cantidad en deportista2.csv: 67785\n",
      "suma de deportista.csv + deportista2.csv: 135572\n",
      "total en deportis_union: 135572\n"
     ]
    }
   ],
   "source": [
    "#validar por cantidades\n",
    "\n",
    "print(f\"cantidad en deportista.csv: {deportista_encabezado.count()}\")\n",
    "print(f\"cantidad en deportista2.csv: {deportista2_datos.count()}\")\n",
    "suma=deportista_encabezado.count() + deportista2_datos.count()\n",
    "print(f\"suma de deportista.csv + deportista2.csv: {suma}\")\n",
    "print(f\"total en deportis_union: {deportis_union.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb0b275f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso', 'equipo_id'],\n",
       " ['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportista_encabezado.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab4cb6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['67787', 'Lee BongJu', '1', '27', '167', '56', '970'],\n",
       " ['67788', 'Lee BuTi', '1', '23', '164', '54', '203'],\n",
       " ['67789', 'Anthony N. Buddy Lee', '1', '34', '172', '62', '1096'],\n",
       " ['67790', 'Alfred A. Butch Lee Porter', '1', '19', '186', '80', '825'],\n",
       " ['67791', 'Lee ByeongGu', '1', '22', '175', '68', '970']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportista2_datos.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "992a19e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso', 'equipo_id'],\n",
       " ['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportis_union.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2fa9a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'equipo', 'sigla'],\n",
       " ['1', '30. Februar', 'AUT'],\n",
       " ['2', 'A North American Team', 'MEX'],\n",
       " ['3', 'Acipactli', 'MEX'],\n",
       " ['4', 'Acturus', 'ARG']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paises.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0b4e8",
   "metadata": {},
   "source": [
    "### Realizamos un join entre \"paises.csv\" con su \"id\" y \"deportis_union\" con su \"equipo_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93a60b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_join_deportis=deportis_union\\\n",
    "    .map(lambda line:[line[-1],line[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45c14afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['equipo_id',\n",
       "  ['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso']],\n",
       " ['199', ['1', 'A Dijiang', '1', '24', '180', '80']],\n",
       " ['199', ['2', 'A Lamusi', '1', '23', '170', '60']],\n",
       " ['273', ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0']],\n",
       " ['278', ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0']],\n",
       " ['705', ['5', 'Christine Jacoba Aaftink', '2', '21', '185', '82']],\n",
       " ['1096', ['6', 'Per Knut Aaland', '1', '31', '188', '75']],\n",
       " ['1096', ['7', 'John Aalberg', '1', '31', '183', '72']],\n",
       " ['705', ['8', 'Cornelia Cor Aalten Strannood ', '2', '18', '168', '0']],\n",
       " ['350', ['9', 'Antti Sami Aalto', '1', '26', '186', '96']],\n",
       " ['350', ['10', 'Einar Ferdinand Einari Aalto', '1', '26', '0', '0']],\n",
       " ['350', ['11', 'Jorma Ilmari Aalto', '1', '22', '182', '76.5']],\n",
       " ['350', ['12', 'Jyri Tapani Aalto', '1', '31', '172', '70']],\n",
       " ['350', ['13', 'Minna Maarit Aalto', '2', '30', '159', '55.5']],\n",
       " ['350', ['14', 'Pirjo Hannele Aalto Mattila ', '2', '32', '171', '65']]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''muestra \"equipo_id\" line[-1], el resto line[:-1]'''\n",
    "paises_join_deportis.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c8926a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuamos con el join\n",
    "paises_join_deportis=deportis_union\\\n",
    "    .map(lambda line:[line[-1],line[:-1]])\\\n",
    "    .join(paises.map(lambda line:[line[0],line[2]]))#quiero el \"id\" y \"sigla\"\n",
    "\n",
    "#Notar que se pierde el encabezado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33318f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('199', (['1', 'A Dijiang', '1', '24', '180', '80'], 'CHN')),\n",
       " ('199', (['2', 'A Lamusi', '1', '23', '170', '60'], 'CHN')),\n",
       " ('199', (['602', 'Abudoureheman', '1', '22', '182', '75'], 'CHN')),\n",
       " ('199', (['1463', 'Ai Linuer', '1', '25', '160', '62'], 'CHN')),\n",
       " ('199', (['1464', 'Ai Yanhan', '2', '14', '168', '54'], 'CHN'))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paises_join_deportis.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fcf3b",
   "metadata": {},
   "source": [
    "## Intento de re-organizar \"paises_join_deportis\" en una lista, sin listas adentro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a23be247",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportis_ordenados=deportis_union\\\n",
    "    .map(lambda line: (line[-1],*line[:-1]))#logré colocar todo en una tupla\n",
    "\n",
    "paises_ordenados=paises.map(lambda line: (line[0], line[2]))\n",
    "\n",
    "join_final=paises_ordenados.join(deportis_ordenados)#no puedo unirlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa8a9f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('equipo_id', 'deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso'),\n",
       " ('199', '1', 'A Dijiang', '1', '24', '180', '80'),\n",
       " ('199', '2', 'A Lamusi', '1', '23', '170', '60'),\n",
       " ('273', '3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0'),\n",
       " ('278', '4', 'Edgar Lindenau Aabye', '1', '34', '0', '0'),\n",
       " ('705', '5', 'Christine Jacoba Aaftink', '2', '21', '185', '82'),\n",
       " ('1096', '6', 'Per Knut Aaland', '1', '31', '188', '75'),\n",
       " ('1096', '7', 'John Aalberg', '1', '31', '183', '72'),\n",
       " ('705', '8', 'Cornelia Cor Aalten Strannood ', '2', '18', '168', '0'),\n",
       " ('350', '9', 'Antti Sami Aalto', '1', '26', '186', '96'),\n",
       " ('350', '10', 'Einar Ferdinand Einari Aalto', '1', '26', '0', '0'),\n",
       " ('350', '11', 'Jorma Ilmari Aalto', '1', '22', '182', '76.5'),\n",
       " ('350', '12', 'Jyri Tapani Aalto', '1', '31', '172', '70'),\n",
       " ('350', '13', 'Minna Maarit Aalto', '2', '30', '159', '55.5'),\n",
       " ('350', '14', 'Pirjo Hannele Aalto Mattila ', '2', '32', '171', '65')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportis_ordenados.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7b71829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'sigla'),\n",
       " ('1', 'AUT'),\n",
       " ('2', 'MEX'),\n",
       " ('3', 'MEX'),\n",
       " ('4', 'ARG'),\n",
       " ('5', 'AFG'),\n",
       " ('6', 'IRL'),\n",
       " ('7', 'SUI'),\n",
       " ('8', 'ALB'),\n",
       " ('9', 'POR'),\n",
       " ('10', 'FRA'),\n",
       " ('11', 'FRA'),\n",
       " ('12', 'ITA'),\n",
       " ('13', 'ITA'),\n",
       " ('14', 'IRL')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paises_ordenados.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3cde1719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('5', ('AFG', '502')),\n",
       " ('5', ('AFG', '1076')),\n",
       " ('5', ('AFG', '1101')),\n",
       " ('5', ('AFG', '1745')),\n",
       " ('5', ('AFG', '4628')),\n",
       " ('5', ('AFG', '5285')),\n",
       " ('5', ('AFG', '5582')),\n",
       " ('5', ('AFG', '5678')),\n",
       " ('5', ('AFG', '5679')),\n",
       " ('5', ('AFG', '5841')),\n",
       " ('5', ('AFG', '5844')),\n",
       " ('5', ('AFG', '6261')),\n",
       " ('5', ('AFG', '6280')),\n",
       " ('5', ('AFG', '6282')),\n",
       " ('5', ('AFG', '6323'))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_final.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3472b",
   "metadata": {},
   "source": [
    "## Fin intento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a4958",
   "metadata": {},
   "source": [
    "### takeSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba905991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('45', (['8843', 'Walter Antonio Bauza', '1', '44', '174', '85'], 'ARG')),\n",
       " ('80', (['21890', 'Eldece ClarkeLewis', '2', '19', '165', '58'], 'BAH')),\n",
       " ('66', (['12968', 'Thomas Patrick Tom Bolger', '1', '23', '0', '0'], 'AUS')),\n",
       " ('970', (['67911', 'Lee HakJa', '2', '23', '165', '58'], 'KOR')),\n",
       " ('982',\n",
       "  (['98732', 'Enrique Quique Ramos Gonzlez', '1', '24', '0', '0'], 'ESP')),\n",
       " ('413',\n",
       "  (['5551', 'Henry Sherard Osborn Ashington', '1', '20', '0', '0'], 'GBR')),\n",
       " ('705',\n",
       "  (['44456', 'Paul Vincent Nicholas Haarhuis', '1', '26', '188', '80'],\n",
       "   'NED')),\n",
       " ('1096',\n",
       "  (['78838', 'Eugene Leroy Roy Mercer', '1', '23', '180', '80'], 'USA')),\n",
       " ('742', (['14449', 'Knut Tore Br', '1', '23', '187', '68'], 'NOR')),\n",
       " ('66', (['8326', 'Clive Barton', '1', '28', '189', '85'], 'AUS')),\n",
       " ('199', (['110045', 'Shu Siyao', '2', '23', '167', '52'], 'CHN')),\n",
       " ('413', (['15576', 'Robert Harry Brown', '1', '34', '0', '0'], 'GBR')),\n",
       " ('656', (['37457', 'Rosa Fuentes', '2', '18', '165', '60'], 'MEX')),\n",
       " ('810', (['134964', 'Edmund Jan Zientara', '1', '31', '170', '68'], 'POL')),\n",
       " ('1096', (['131389', 'John Albert Wolters', '1', '19', '181', '81'], 'USA'))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takeSample(repetidos,cantidad_datos,semilla_aleatorea), la semilla_aleatorea es la muestra a tomar\n",
    "paises_join_deportis.takeSample(False,15,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25917788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('999', (['92679', 'Trygve Bjarne Pedersen', '1', '35', '0', '0'], 'NOR')),\n",
       " ('999', (['1144', 'Henrik Agersborg', '1', '47', '0', '0'], 'NOR'))]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paises_join_deportis.top(2)#Ojo se perdió el encabezado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f409a54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['resultado_id', 'medalla', 'deportista_id', 'juego_id', 'evento_id'],\n",
       " ['1', 'NA', '1', '39', '1'],\n",
       " ['2', 'NA', '2', '49', '2'],\n",
       " ['3', 'NA', '3', '7', '3'],\n",
       " ['4', 'Gold', '4', '2', '4'],\n",
       " ['5', 'NA', '5', '36', '5'],\n",
       " ['6', 'NA', '5', '36', '6'],\n",
       " ['7', 'NA', '5', '38', '5'],\n",
       " ['8', 'NA', '5', '38', '6'],\n",
       " ['9', 'NA', '5', '40', '5'],\n",
       " ['10', 'NA', '5', '40', '6'],\n",
       " ['11', 'NA', '6', '38', '7'],\n",
       " ['12', 'NA', '6', '38', '8'],\n",
       " ['13', 'NA', '6', '38', '9'],\n",
       " ['14', 'NA', '6', '38', '10']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e6677121",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados=resultados.filter(lambda line: \"NA\" not in line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca86d160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['resultado_id', 'medalla', 'deportista_id', 'juego_id', 'evento_id'],\n",
       " ['4', 'Gold', '4', '2', '4'],\n",
       " ['38', 'Bronze', '15', '7', '19'],\n",
       " ['39', 'Bronze', '15', '7', '20'],\n",
       " ['41', 'Bronze', '16', '50', '14'],\n",
       " ['42', 'Bronze', '17', '17', '21'],\n",
       " ['43', 'Gold', '17', '17', '22'],\n",
       " ['45', 'Gold', '17', '17', '24'],\n",
       " ['49', 'Gold', '17', '17', '28'],\n",
       " ['51', 'Bronze', '17', '19', '22'],\n",
       " ['61', 'Gold', '20', '38', '32'],\n",
       " ['62', 'Bronze', '20', '38', '33'],\n",
       " ['64', 'Silver', '20', '40', '31'],\n",
       " ['65', 'Bronze', '20', '40', '32'],\n",
       " ['68', 'Silver', '20', '40', '35']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8f1a9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deportista_id'],\n",
       " ['4'],\n",
       " ['15'],\n",
       " ['15'],\n",
       " ['16'],\n",
       " ['17'],\n",
       " ['17'],\n",
       " ['17'],\n",
       " ['17'],\n",
       " ['17'],\n",
       " ['20'],\n",
       " ['20'],\n",
       " ['20'],\n",
       " ['20'],\n",
       " ['20']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.map(lambda line: [line[2]])\\\n",
    "    .take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "85b41354",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''deportista_medallas=paises_join_deportis\\\n",
    "    .map(lambda line: [line[1][0][0]])\\\n",
    "    .join(resultados.map(lambda line: [line[2]]))'''\n",
    "\n",
    "deportista_medallas=paises_join_deportis\\\n",
    "    .join(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7dad177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1090',\n",
       "  ((['9150', 'Yelena Yuryevna Bechke Petrova Ellis ', '2', '26', '158', '48'],\n",
       "    'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['9819', 'Gennady Vladimirovich Belyakov', '1', '23', '171', '84'], 'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['41896', 'Andrey Vladimirovich Gorokhov', '1', '23', '185', '92'], 'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['64345', 'Sergey Valeryevich Kruglov', '1', '31', '0', '0'], 'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['70906', 'Igor Vladimirovich Lobanov', '1', '22', '181', '78'], 'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['90775', 'Irina Vladimirovna Palina', '2', '22', '162', '60'], 'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['91898', 'Aleksandr Pashkov', '1', '38', '190', '90'], 'EUN'), 'Bronze')),\n",
       " ('1090',\n",
       "  ((['94109', 'Denis Alekseyevich Petrov', '1', '23', '182', '77'], 'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['94120', 'Oleg Yuryevich Petrov', '1', '24', '175', '95'], 'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['116299', 'Oleg Borisovich Sukhoruchenko', '1', '26', '180', '83'],\n",
       "    'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['123586', 'Maiya Valentinovna Usova Orletskaya ', '2', '27', '160', '45'],\n",
       "    'EUN'),\n",
       "   'Bronze')),\n",
       " ('1090',\n",
       "  ((['134888',\n",
       "     'Aleksandr Vyacheslavovich Sasha Zhulin',\n",
       "     '1',\n",
       "     '28',\n",
       "     '176',\n",
       "     '70'],\n",
       "    'EUN'),\n",
       "   'Bronze')),\n",
       " ('160',\n",
       "  ((['11633', 'Beni Bertrand Binobagira', '1', '23', '176', '69'], 'BDI'),\n",
       "   'Bronze')),\n",
       " ('160',\n",
       "  ((['37904', 'JeanPaul Gahimbar', '1', '33', '174', '57'], 'BDI'), 'Bronze')),\n",
       " ('160',\n",
       "  ((['37966', 'Antoine Gakeme', '1', '24', '169', '60'], 'BDI'), 'Bronze'))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportista_medallas.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f378a34",
   "metadata": {},
   "source": [
    "### <a name=\"mark_08\"></a> Operaciones numéricas.\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4c1fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_por_medallas={\n",
    "    'Gold': 7,\n",
    "    'Silver': 5,\n",
    "    'Bronze':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d9add68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizando extracción:\n",
    "paises_medallas=deportista_medallas.map(lambda line: (line[1][0][1], valores_por_medallas[line[1][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3fad511d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('EUN', 4),\n",
       " ('BDI', 4),\n",
       " ('BDI', 4),\n",
       " ('BDI', 4)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paises_medallas.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe8463a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add #nos permite sumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9350074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'reduceByKey(add): Aplicar reduceByKey con add significa que deseas agregar los valores (números de medallas) que tienen \\nla misma clave (país). En este caso, add es una función que simplemente suma dos números.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conclusion=paises_medallas.reduceByKey(add)\\\n",
    "    .sortBy(lambda line: line[1], ascending=False)\n",
    "\n",
    "'''reduceByKey(add): Aplicar reduceByKey con add significa que deseas agregar los valores (números de medallas) que tienen \n",
    "la misma clave (país). En este caso, add es una función que simplemente suma dos números.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "53be7382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CAN', 32538),\n",
       " ('ARG', 12520),\n",
       " ('HUN', 10860),\n",
       " ('MEX', 6124),\n",
       " ('RSA', 3788),\n",
       " ('BLR', 3580),\n",
       " ('LTU', 1535),\n",
       " ('MGL', 1460),\n",
       " ('USA', 1342),\n",
       " ('AZE', 1218)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conclusion.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "65881c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_03.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e59081",
   "metadata": {},
   "source": [
    "### <a name=\"mark_09\"></a> DataFrames.\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_19.png)\n",
    "**-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec36a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType,FloatType\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "71565990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu_dell_ubuntu_01/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark=SparkContext(master=\"local\", appName=\"DataFrames\")\n",
    "sqlContext=SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "93e4ba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14520\r\n",
      "drwxr-xr-x 2 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  9 09:38 .\r\n",
      "drwxr-xr-x 5 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  7 13:11 ..\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01     946 Oct  7 13:11 deporte.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2764536 Oct  7 13:11 deportista.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2807091 Oct  7 13:11 deportista2.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2776782 Oct  7 13:11 deportistaError.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   33262 Oct  7 13:11 evento.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    1978 Oct  7 13:11 juegos.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01  255853 Oct  7 13:11 modelo_relacional.jpg\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   23606 Oct  7 13:11 paises.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 6172796 Oct  7 13:11 resultados.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75ed4763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",nombre_juego,annio,temporada,ciudad\r\n",
      "1,1896 Verano,1896,Verano,Athina\r\n",
      "2,1900 Verano,1900,Verano,Paris\r\n",
      "3,1904 Verano,1904,Verano,St. Louis\r\n",
      "4,1906 Verano,1906,Verano,Athina\r\n"
     ]
    }
   ],
   "source": [
    "#Analizando archivo desde Linux\n",
    "!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/juegos.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2df0b",
   "metadata": {},
   "source": [
    "### Antes de cargar el archivo primero creamos el Schema, utilizamos StructFile() para definir el nombre y tipo de columna dentro de StructType().\n",
    "- StructFile(\"nombre_columna\", tipo_dato, ¿es_obligatorio?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3248c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "juegoSchema = StructType([\n",
    "    StructField(\"juego_id\", IntegerType(), False),\n",
    "    StructField(\"nombre_juego\", StringType(), False),\n",
    "    StructField(\"anio\", StringType(), False),\n",
    "    StructField(\"temporada\", StringType(), False),\n",
    "    StructField(\"ciudad\", StringType(), False)\n",
    "])\n",
    "\n",
    "#Cargando el archivo\n",
    "path=\"./curso_spark_git_clone/curso-apache-spark-platzi/files/\"\n",
    "\n",
    "juegoDF=sqlContext.read.schema(juegoSchema)\\\n",
    "    .option(\"header\", \"true\").csv(path+\"juegos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8d8074ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/23 14:17:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , nombre_juego, annio, temporada, ciudad\n",
      " Schema: juego_id, nombre_juego, anio, temporada, ciudad\n",
      "Expected: juego_id but found: \n",
      "CSV file: file:///home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/juegos.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(juego_id=1, nombre_juego='1896 Verano', anio='1896', temporada='Verano', ciudad='Athina'),\n",
       " Row(juego_id=2, nombre_juego='1900 Verano', anio='1900', temporada='Verano', ciudad='Paris'),\n",
       " Row(juego_id=3, nombre_juego='1904 Verano', anio='1904', temporada='Verano', ciudad='St. Louis'),\n",
       " Row(juego_id=4, nombre_juego='1906 Verano', anio='1906', temporada='Verano', ciudad='Athina'),\n",
       " Row(juego_id=5, nombre_juego='1908 Verano', anio='1908', temporada='Verano', ciudad='London')]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "juegoDF.take(5)#La vista no es muy agradable, por eso usamos show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad1512d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+---------+--------------------+\n",
      "|juego_id| nombre_juego|anio|temporada|              ciudad|\n",
      "+--------+-------------+----+---------+--------------------+\n",
      "|       1|  1896 Verano|1896|   Verano|              Athina|\n",
      "|       2|  1900 Verano|1900|   Verano|               Paris|\n",
      "|       3|  1904 Verano|1904|   Verano|           St. Louis|\n",
      "|       4|  1906 Verano|1906|   Verano|              Athina|\n",
      "|       5|  1908 Verano|1908|   Verano|              London|\n",
      "|       6|  1912 Verano|1912|   Verano|           Stockholm|\n",
      "|       7|  1920 Verano|1920|   Verano|           Antwerpen|\n",
      "|       8|1924 Invierno|1924| Invierno|            Chamonix|\n",
      "|       9|  1924 Verano|1924|   Verano|               Paris|\n",
      "|      10|1928 Invierno|1928| Invierno|        Sankt Moritz|\n",
      "|      11|  1928 Verano|1928|   Verano|           Amsterdam|\n",
      "|      12|1932 Invierno|1932| Invierno|         Lake Placid|\n",
      "|      13|  1932 Verano|1932|   Verano|         Los Angeles|\n",
      "|      14|1936 Invierno|1936| Invierno|Garmisch-Partenki...|\n",
      "|      15|  1936 Verano|1936|   Verano|              Berlin|\n",
      "+--------+-------------+----+---------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/23 14:17:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , nombre_juego, annio, temporada, ciudad\n",
      " Schema: juego_id, nombre_juego, anio, temporada, ciudad\n",
      "Expected: juego_id but found: \n",
      "CSV file: file:///home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/juegos.csv\n"
     ]
    }
   ],
   "source": [
    "juegoDF.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c498ec8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d24f4f",
   "metadata": {},
   "source": [
    "### Si revisamos los \"jobs --> DAGs\" podemos ver como el \"Agente\" de Spark (optimizador) realizó distintas acciones.\n",
    "\n",
    "![](img_20.png)\n",
    "\n",
    "- Scan csv --> Escanea el archivo\n",
    "- WholeStageCodegen --> Generación de código interna (chequeo de código)\n",
    "- mapPartitionsInternal --> Escaneo de las particiones\n",
    "\n",
    "**-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "236d1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de8f0d",
   "metadata": {},
   "source": [
    "### <a name=\"mark_10\"></a> Inferencia tipos de datos.\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- Vamos a trabajar a partir de un RDD pasandolo a un DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6866ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015835b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 09:53:08 WARN Utils: Your hostname, CompuDell01 resolves to a loopback address: 127.0.1.1; using 192.168.29.145 instead (on interface eth0)\n",
      "23/10/26 09:53:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/26 09:53:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc_04=SparkContext(master=\"local\", appName=\"new_rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d811376",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./curso_spark_git_clone/curso-apache-spark-platzi/files/\"\n",
    "\n",
    "deportis_encabezado=sc_04.textFile(path+\"deportista.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "deportis_datos=sc_04.textFile(path+\"deportista2.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "datos_deportis=deportis_encabezado.union(deportis_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90121d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso', 'equipo_id'],\n",
       " ['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_deportis.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d7bb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\niter() devuelve valor a valor lo que procesamos\\nlist(iterador)[:1] tomará todo el rdd menos la primer lista\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos una función para eliminar el encabezado.\n",
    "def elimna_encabezado(indice, iterador):\n",
    "    return iter(list(iterador)[1:])\n",
    "'''\n",
    "iter() devuelve valor a valor lo que procesamos\n",
    "list(iterador)[:1] tomará todo el rdd menos la primer lista\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91594ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmapPartitionsWithIndex() a la función aplicada (elimna_encabezado), se le va a pasar 2 parámetros,\\nprimero, toda la columna y segundo, un valor por índice\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_deportis=datos_deportis.mapPartitionsWithIndex(elimna_encabezado)\n",
    "'''\n",
    "mapPartitionsWithIndex() a la función aplicada (elimna_encabezado), se le va a pasar 2 parámetros,\n",
    "primero, toda la columna y segundo, un valor por índice\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2abd419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278'],\n",
       " ['5', 'Christine Jacoba Aaftink', '2', '21', '185', '82', '705']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rdd sin encabezado\n",
    "datos_deportis.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dabb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antes de transformar el RDD hay que transformar los datos del RDD.\n",
    "datos_deportis=datos_deportis.map(lambda line: (\n",
    "    int(line[0]),\n",
    "    line[1], #queda como str\n",
    "    int(line[2]),\n",
    "    int(line[3]),\n",
    "    float(line[4]),#es la columna de altura\n",
    "    float(line[5]),#es la columna de peso\n",
    "    int(line[6])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7da210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType,FloatType\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f351410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creando el schema \n",
    "schema = StructType([\n",
    "    StructField(\"deportista_id\", IntegerType(), False),\n",
    "    StructField(\"nombre\", StringType(),False),\n",
    "    StructField(\"genero\", IntegerType(),False),\n",
    "    StructField(\"edad\", IntegerType(),False),\n",
    "    StructField(\"altura\",FloatType(),False),\n",
    "    StructField(\"peso\",FloatType(),False),\n",
    "    StructField(\"equipo_id\",IntegerType(),False)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9a7e96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu_dell_ubuntu_01/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#aplicando el schema al rdd, SQLContext() nos permite crear un DF a partir de nuestro RDD y schema\n",
    "sqlContext=SQLContext(sc_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35641353",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportis_df=sqlContext.createDataFrame(datos_deportis, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06c8230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24| 180.0|80.0|      199|\n",
      "|            2|            A Lamusi|     1|  23| 170.0|60.0|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|   0.0| 0.0|      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|   0.0| 0.0|      278|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|\n",
      "|            7|        John Aalberg|     1|  31| 183.0|72.0|     1096|\n",
      "|            8|Cornelia Cor Aalt...|     2|  18| 168.0| 0.0|      705|\n",
      "|            9|    Antti Sami Aalto|     1|  26| 186.0|96.0|      350|\n",
      "|           10|Einar Ferdinand E...|     1|  26|   0.0| 0.0|      350|\n",
      "|           11|  Jorma Ilmari Aalto|     1|  22| 182.0|76.5|      350|\n",
      "|           12|   Jyri Tapani Aalto|     1|  31| 172.0|70.0|      350|\n",
      "|           13|  Minna Maarit Aalto|     2|  30| 159.0|55.5|      350|\n",
      "|           14|Pirjo Hannele Aal...|     2|  32| 171.0|65.0|      350|\n",
      "|           15|Arvo Ossian Aaltonen|     1|  22|   0.0| 0.0|      350|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b73a92",
   "metadata": {},
   "source": [
    "### Otra forma muy sencilla de inferir un schema en una línea que da un resultado muy parecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63bda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportis_ts = sqlContext.read.csv(path+\"deportista.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff279e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24|   180|80.0|      199|\n",
      "|            2|            A Lamusi|     1|  23|   170|60.0|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|     0| 0.0|      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|     0| 0.0|      278|\n",
      "|            5|Christine Jacoba ...|     2|  21|   185|82.0|      705|\n",
      "|            6|     Per Knut Aaland|     1|  31|   188|75.0|     1096|\n",
      "|            7|        John Aalberg|     1|  31|   183|72.0|     1096|\n",
      "|            8|Cornelia Cor Aalt...|     2|  18|   168| 0.0|      705|\n",
      "|            9|    Antti Sami Aalto|     1|  26|   186|96.0|      350|\n",
      "|           10|Einar Ferdinand E...|     1|  26|     0| 0.0|      350|\n",
      "|           11|  Jorma Ilmari Aalto|     1|  22|   182|76.5|      350|\n",
      "|           12|   Jyri Tapani Aalto|     1|  31|   172|70.0|      350|\n",
      "|           13|  Minna Maarit Aalto|     2|  30|   159|55.5|      350|\n",
      "|           14|Pirjo Hannele Aal...|     2|  32|   171|65.0|      350|\n",
      "|           15|Arvo Ossian Aaltonen|     1|  22|     0| 0.0|      350|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_ts.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a5787",
   "metadata": {},
   "source": [
    "### Comparando los schemas podemos observar esas diferencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b06c3bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = false)\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- genero: integer (nullable = false)\n",
      " |-- edad: integer (nullable = false)\n",
      " |-- altura: float (nullable = false)\n",
      " |-- peso: float (nullable = false)\n",
      " |-- equipo_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02632e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- genero: integer (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- altura: integer (nullable = true)\n",
      " |-- peso: double (nullable = true)\n",
      " |-- equipo_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_ts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8e924",
   "metadata": {},
   "source": [
    "### Pasando de RDD a DF los demas archivos.\n",
    "- deporte.csv\n",
    "- deportistaError.csv\n",
    "- evento.csv\n",
    "- juegos.csv\n",
    "- paises.csv\n",
    "- resultados.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22508408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14520\r\n",
      "drwxr-xr-x 2 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  9 09:38 .\r\n",
      "drwxr-xr-x 5 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  7 13:11 ..\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01     946 Oct  7 13:11 deporte.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2764536 Oct  7 13:11 deportista.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2807091 Oct  7 13:11 deportista2.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2776782 Oct  7 13:11 deportistaError.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   33262 Oct  7 13:11 evento.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    1978 Oct  7 13:11 juegos.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01  255853 Oct  7 13:11 modelo_relacional.jpg\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   23606 Oct  7 13:11 paises.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 6172795 Oct 24 09:55 resultados.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81191c05",
   "metadata": {},
   "source": [
    "### deportes.csv desde RDD a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2d7c1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultado_id,medalla,deportista_id,juego_id,evento_id\r\n",
      "1,NA,1,39,1\r\n",
      "2,NA,2,49,2\r\n",
      "3,NA,3,7,3\r\n",
      "4,Gold,4,2,4\r\n"
     ]
    }
   ],
   "source": [
    "#!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/deporte.csv\n",
    "#!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/deportistaError.csv\n",
    "#!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/evento.csv\n",
    "#!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/juegos.csv\n",
    "#!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/paises.csv\n",
    "!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/resultados.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27c4fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creando los rdd\n",
    "deporte_rdd=sc_04.textFile(path+\"deporte.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "deportistaError_rdd=sc_04.textFile(path+\"deportistaError.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "evento_rdd=sc_04.textFile(path+\"evento.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "juegos_rdd=sc_04.textFile(path+\"juegos.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "paises_rdd=sc_04.textFile(path+\"paises.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "resultados_rdd=sc_04.textFile(path+\"resultados.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "320a7784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['resultado_id', 'medalla', 'deportista_id', 'juego_id', 'evento_id'],\n",
       " ['1', 'NA', '1', '39', '1'],\n",
       " ['2', 'NA', '2', '49', '2'],\n",
       " ['3', 'NA', '3', '7', '3'],\n",
       " ['4', 'Gold', '4', '2', '4']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2935543f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmapPartitionsWithIndex() a la función aplicada se le paso 2 parámetros, toda la columna y un valor\\npor índice\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#utilizando la función eliminar_encabezado\n",
    "\n",
    "deporte_rdd_sin_encabezado=deporte_rdd.mapPartitionsWithIndex(elimna_encabezado)\n",
    "\n",
    "deportistaError_rdd_sin_encabezado=deportistaError_rdd.mapPartitionsWithIndex(elimna_encabezado)\n",
    "\n",
    "evento_rdd_sin_encabezado=evento_rdd.mapPartitionsWithIndex(elimna_encabezado)\n",
    "\n",
    "juegos_rdd_sin_encabezado=juegos_rdd.mapPartitionsWithIndex(elimna_encabezado)\n",
    "\n",
    "paises_rdd_sin_encabezado=paises_rdd.mapPartitionsWithIndex(elimna_encabezado)\n",
    "\n",
    "resultados_rdd_sin_encabezado=resultados_rdd.mapPartitionsWithIndex(elimna_encabezado)\n",
    "\n",
    "'''\n",
    "mapPartitionsWithIndex() a la función aplicada se le paso 2 parámetros, toda la columna y un valor\n",
    "por índice\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "716d1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antes de transformar el RDD hay que transformar los datos del RDD.\n",
    "\n",
    "deporte_rdd_sin_encabezado=deporte_rdd_sin_encabezado.map(lambda line: (\n",
    "    int(line[0]),\n",
    "    line[1] #queda como str\n",
    "))\n",
    "\n",
    "#En line[4] y line[5] se transforman los valores vacios '' en 0\n",
    "deportistaError_rdd_sin_encabezado=deportistaError_rdd_sin_encabezado.map(lambda line:( \n",
    "    int(line[0]),\n",
    "    line[1],\n",
    "    int(line[2]),\n",
    "    int(line[3]),\n",
    "    float(line[4]) if line[4]!='' else 0.0,\n",
    "    float(line[5]) if line[5]!='' else 0.0,\n",
    "    int(line[6]) \n",
    "))\n",
    " #line[1]+line[2] if len(line) == 4 else (line[1]+line[2]+line[3] if len(line) == 5 else line[1]),\n",
    "evento_rdd_sin_encabezado=evento_rdd_sin_encabezado.map(lambda line:(\n",
    "    int(line[0]),\n",
    "    line[1]+line[2] if len(line) > 3 else line[1],\n",
    "    int(line[3]) if len(line) > 3 else int(line[2])\n",
    "    \n",
    "))\n",
    "'''\n",
    "Hay un error en el archivo original \"evento_rdd\"\n",
    "[['evento_id', 'evento', 'deporte_id'],\n",
    " ['1', \"Basketball Men's Basketball\", '1'],\n",
    " ['2', \"Judo Men's Extra-Lightweight\", '2'],\n",
    " ['3', \"Football Men's Football\", '3'],\n",
    " ['4', \"Tug-Of-War Men's Tug-Of-War\", '4'],\n",
    " ['5', \"Speed Skating Women's 500 metres\", '5'],\n",
    " ['6', '\"Speed Skating Women\\'s 1', '000 metres\"', '5'], esta última lista contiene una \",\" de más\n",
    " lo que no es correcto, entonces el código identifica si la lista tiene una logitud mayor a 3 \n",
    " , si es así une line[1] con line[2], y luego para la tercer columna utiliza un indice line[3] \n",
    " para completar correctamente el RDD.\n",
    "'''\n",
    "juegos_rdd_sin_encabezado=juegos_rdd_sin_encabezado.map(lambda line:(\n",
    "    int(line[0]),\n",
    "    line[1],\n",
    "    int(line[2]),\n",
    "    line[3],\n",
    "    line[4]\n",
    "))\n",
    "\n",
    "paises_rdd_sin_encabezado=paises_rdd_sin_encabezado.map(lambda line:(\n",
    "    int(line[0]),\n",
    "    line[1],\n",
    "    line[2]    \n",
    "))\n",
    "\n",
    "resultados_rdd_sin_encabezado=resultados_rdd_sin_encabezado.map(lambda line:(\n",
    "    int(line[0]),\n",
    "    line[1],\n",
    "    int(line[2]),\n",
    "    int(line[3]),\n",
    "    int(line[4])    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d214996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'NA', 1, 39, 1),\n",
       " (2, 'NA', 2, 49, 2),\n",
       " (3, 'NA', 3, 7, 3),\n",
       " (4, 'Gold', 4, 2, 4),\n",
       " (5, 'NA', 5, 36, 5)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_rdd_sin_encabezado.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "819a8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creando el schema \n",
    "schema_deporte_rdd = StructType([\n",
    "    StructField(\"deporte_id\", IntegerType(), False),\n",
    "    StructField(\"deporte\", StringType(),False)    \n",
    "])\n",
    "\n",
    "schema_deportistaError_rdd=StructType([\n",
    "    StructField(\"deportista_id\", IntegerType(),False),\n",
    "    StructField(\"nombre\", StringType(),False),\n",
    "    StructField(\"genero\", IntegerType(),False),\n",
    "    StructField(\"edad\",IntegerType(),False),\n",
    "    StructField(\"altura\",FloatType(),False),\n",
    "    StructField(\"peso\",FloatType(),False),\n",
    "    StructField(\"equipo_id\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "schema_evento_rdd=StructType([\n",
    "    StructField(\"evento_id\",IntegerType(),False),\n",
    "    StructField(\"evento\",StringType(),False),\n",
    "    StructField(\"deporte_id\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "schema_juegos_rdd=StructType([\n",
    "    StructField(\"juego_id\",IntegerType(),False),\n",
    "    StructField(\"nombre_juego\",StringType(),False),\n",
    "    StructField(\"anio\",IntegerType(),False),\n",
    "    StructField(\"temporada\",StringType(),False),\n",
    "    StructField(\"ciudad\",StringType(),False)\n",
    "])\n",
    "\n",
    "schema_paises_rdd=StructType([\n",
    "    StructField(\"pais_id\",IntegerType(),False),\n",
    "    StructField(\"equipo\",StringType(),False),\n",
    "    StructField(\"sigla\",StringType(),False)\n",
    "])\n",
    "\n",
    "schema_resultados_rdd=StructType([\n",
    "    StructField(\"resultado_id\", IntegerType(),False),\n",
    "    StructField(\"medalla\",StringType(),False),\n",
    "    StructField(\"deportista_id\",IntegerType(),False),\n",
    "    StructField(\"juego_id\",IntegerType(),False),\n",
    "    StructField(\"evento_id\",IntegerType(),False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24862ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "deporte_df=sqlContext.createDataFrame(deporte_rdd_sin_encabezado, schema_deporte_rdd)\n",
    "\n",
    "deportistaError_df=sqlContext.createDataFrame(deportistaError_rdd_sin_encabezado, schema_deportistaError_rdd)\n",
    "\n",
    "evento_df=sqlContext.createDataFrame(evento_rdd_sin_encabezado, schema_evento_rdd)\n",
    "\n",
    "juegos_df=sqlContext.createDataFrame(juegos_rdd_sin_encabezado, schema_juegos_rdd)\n",
    "\n",
    "paises_df=sqlContext.createDataFrame(paises_rdd_sin_encabezado, schema_paises_rdd)\n",
    "\n",
    "resultados_df=sqlContext.createDataFrame(resultados_rdd_sin_encabezado, schema_resultados_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2dddad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+--------+---------+\n",
      "|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|           1|     NA|            1|      39|        1|\n",
      "|           2|     NA|            2|      49|        2|\n",
      "|           3|     NA|            3|       7|        3|\n",
      "|           4|   Gold|            4|       2|        4|\n",
      "|           5|     NA|            5|      36|        5|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deportis_df.show(5)\n",
    "#deporte_df.show(5)\n",
    "#deportistaError_df.show(5)\n",
    "#evento_df.show(5)\n",
    "#juegos_df.show(5)\n",
    "#paises_df.show(5)\n",
    "resultados_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceff71a",
   "metadata": {},
   "source": [
    "### <a name=\"mark_11\"></a> Operaciones sobre DF.\n",
    "\n",
    "### <a name=\"mark_12\"></a> printSchema().\n",
    "\n",
    "### [Index](#index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e82386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = false)\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- genero: integer (nullable = false)\n",
      " |-- edad: integer (nullable = false)\n",
      " |-- altura: float (nullable = false)\n",
      " |-- peso: float (nullable = false)\n",
      " |-- equipo_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94057819",
   "metadata": {},
   "source": [
    "### <a name=\"mark_13\"></a> withColumnRenamed(\"current_name\", \"new_name\").drop(\"column_name\")\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- Nota_01: Se combinan \"withColumnRenamed()\" y \"drop()\"\n",
    "- Nota_02: Esto genera un nuevo df, no modifica el original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0982bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quiero reemplazar los nombre de ciertas columnas.\n",
    "\n",
    "deportis_df_01=deportis_df.withColumnRenamed(\"genero\", \"sexo\").drop(\"altura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340b6038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = false)\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- sexo: integer (nullable = false)\n",
      " |-- edad: integer (nullable = false)\n",
      " |-- peso: float (nullable = false)\n",
      " |-- equipo_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df_01.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3d392",
   "metadata": {},
   "source": [
    "### <a name=\"mark_14\"></a> select(), col(\"column_name\").alias(\"new_name\")\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- Para poder utilizar col() necesitamos importar desde sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11e27731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0bbf0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportis_df_02=deportis_df_01.select(\n",
    "    \"deportista_id\", \n",
    "    \"nombre\",\n",
    "    f.col(\"edad\").alias(\"edad_jugador\"),\n",
    "    \"equipo_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf59ffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------+---------+\n",
      "|deportista_id|              nombre|edad_jugador|equipo_id|\n",
      "+-------------+--------------------+------------+---------+\n",
      "|            1|           A Dijiang|          24|      199|\n",
      "|            2|            A Lamusi|          23|      199|\n",
      "|            3| Gunnar Nielsen Aaby|          24|      273|\n",
      "|            4|Edgar Lindenau Aabye|          34|      278|\n",
      "|            5|Christine Jacoba ...|          21|      705|\n",
      "|            6|     Per Knut Aaland|          31|     1096|\n",
      "|            7|        John Aalberg|          31|     1096|\n",
      "|            8|Cornelia Cor Aalt...|          18|      705|\n",
      "|            9|    Antti Sami Aalto|          26|      350|\n",
      "|           10|Einar Ferdinand E...|          26|      350|\n",
      "|           11|  Jorma Ilmari Aalto|          22|      350|\n",
      "|           12|   Jyri Tapani Aalto|          31|      350|\n",
      "|           13|  Minna Maarit Aalto|          30|      350|\n",
      "|           14|Pirjo Hannele Aal...|          32|      350|\n",
      "|           15|Arvo Ossian Aaltonen|          22|      350|\n",
      "+-------------+--------------------+------------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df_02.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda63237",
   "metadata": {},
   "source": [
    "### <a name=\"mark_15\"></a> sort(\"column_name\")\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75e91308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------+---------+\n",
      "|deportista_id|              nombre|edad_jugador|equipo_id|\n",
      "+-------------+--------------------+------------+---------+\n",
      "|          133|           Franz Abb|           0|      399|\n",
      "|        67900|      Lee GyeongSeop|           0|      970|\n",
      "|          167|Ould Lamine Abdallah|           0|      362|\n",
      "|        67991|        Lee JeongGyu|           0|      970|\n",
      "|           66|     Mohamed Abakkar|           0|     1003|\n",
      "+-------------+--------------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deportis_df_02.sort(\"edad_jugador\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad5fea",
   "metadata": {},
   "source": [
    "### <a name=\"mark_16\"></a> filter(df.column_name logic_condition)\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f9955e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------+---------+\n",
      "|deportista_id|              nombre|edad_jugador|equipo_id|\n",
      "+-------------+--------------------+------------+---------+\n",
      "|            1|           A Dijiang|          24|      199|\n",
      "|            2|            A Lamusi|          23|      199|\n",
      "|            3| Gunnar Nielsen Aaby|          24|      273|\n",
      "|            4|Edgar Lindenau Aabye|          34|      278|\n",
      "|            5|Christine Jacoba ...|          21|      705|\n",
      "+-------------+--------------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df_02.filter(deportis_df_02.edad_jugador != 0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63d0630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+------------+---------+\n",
      "|deportista_id|             nombre|edad_jugador|equipo_id|\n",
      "+-------------+-------------------+------------+---------+\n",
      "|            1|          A Dijiang|          24|      199|\n",
      "|            3|Gunnar Nielsen Aaby|          24|      273|\n",
      "|           24|   Nils Egil Aaness|          24|      742|\n",
      "|           25|   Alf Lied Aanning|          24|      742|\n",
      "|           31|    Evald rma rman |          24|      331|\n",
      "+-------------+-------------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df_02.filter(\n",
    "    (deportis_df_02.edad_jugador != 0) & (deportis_df_02.edad_jugador == 24)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd94eb",
   "metadata": {},
   "source": [
    "### <a name=\"mark_17\"></a> Agrupaciones y operaciones join, multiple joins, select() sobre DF\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- Analizamos los schemas para realizar los joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1038a908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = false)\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- genero: integer (nullable = false)\n",
      " |-- edad: integer (nullable = false)\n",
      " |-- altura: float (nullable = false)\n",
      " |-- peso: float (nullable = false)\n",
      " |-- equipo_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "779f92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- resultado_id: integer (nullable = false)\n",
      " |-- medalla: string (nullable = false)\n",
      " |-- deportista_id: integer (nullable = false)\n",
      " |-- juego_id: integer (nullable = false)\n",
      " |-- evento_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "72cc08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_df_mod=resultados_df.withColumn(\"medalla\", f.when(resultados_df[\"medalla\"] == 'NA', 'sin medalla').otherwise(resultados_df[\"medalla\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f71ffc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+--------+---------+\n",
      "|resultado_id|    medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-----------+-------------+--------+---------+\n",
      "|           1|sin medalla|            1|      39|        1|\n",
      "|           2|sin medalla|            2|      49|        2|\n",
      "|           3|sin medalla|            3|       7|        3|\n",
      "|           4|       Gold|            4|       2|        4|\n",
      "|           5|sin medalla|            5|      36|        5|\n",
      "+------------+-----------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados_df_mod.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "433e5f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad \"deportis_df:\" 135570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad \"resultados_df:\" 271116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAl momento de asignar los tipos de datos a rdd, \"resultados_rdd\" se rompió para algunas funciones de agregación,\\ncomo count()\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'cantidad \"deportis_df:\" {deportis_df.count()}')\n",
    "print(f'cantidad \"resultados_df:\" {resultados_df_mod.count()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "547641b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+--------+---------+-------------+--------------------+------+----+------+----+---------+\n",
      "|resultado_id|    medalla|deportista_id|juego_id|evento_id|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+------------+-----------+-------------+--------+---------+-------------+--------------------+------+----+------+----+---------+\n",
      "|           1|sin medalla|            1|      39|        1|            1|           A Dijiang|     1|  24| 180.0|80.0|      199|\n",
      "|           2|sin medalla|            2|      49|        2|            2|            A Lamusi|     1|  23| 170.0|60.0|      199|\n",
      "|           3|sin medalla|            3|       7|        3|            3| Gunnar Nielsen Aaby|     1|  24|   0.0| 0.0|      273|\n",
      "|           4|       Gold|            4|       2|        4|            4|Edgar Lindenau Aabye|     1|  34|   0.0| 0.0|      278|\n",
      "|           5|sin medalla|            5|      36|        5|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|\n",
      "|           6|sin medalla|            5|      36|        6|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|\n",
      "|           7|sin medalla|            5|      38|        5|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|\n",
      "|           8|sin medalla|            5|      38|        6|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|\n",
      "|           9|sin medalla|            5|      40|        5|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|\n",
      "|          10|sin medalla|            5|      40|        6|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|\n",
      "|          11|sin medalla|            6|      38|        7|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|\n",
      "|          12|sin medalla|            6|      38|        8|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|\n",
      "|          13|sin medalla|            6|      38|        9|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|\n",
      "|          14|sin medalla|            6|      38|       10|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|\n",
      "|          15|sin medalla|            6|      40|        7|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|\n",
      "+------------+-----------+-------------+--------+---------+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 80:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Una prueba realizando un join desde \"resultados_df_mod\" hacia \"deportis_df\" muestra que fuciona correctamente \n",
    "con un \"left\" pero no con un \"right\", se revisaron los csv encontrando valores \"#N/A\" en varias posiciones, reemplazando\n",
    "esto valores, se solucionó el inconveniente.\n",
    "'''\n",
    "\n",
    "resultados_df_mod.join(deportis_df, resultados_df_mod.deportista_id==deportis_df.deportista_id, \"right\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fd1aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- juego_id: integer (nullable = false)\n",
      " |-- nombre_juego: string (nullable = false)\n",
      " |-- anio: integer (nullable = false)\n",
      " |-- temporada: string (nullable = false)\n",
      " |-- ciudad: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "juegos_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3648959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- evento_id: integer (nullable = false)\n",
      " |-- evento: string (nullable = false)\n",
      " |-- deporte_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evento_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d525bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+\n",
      "|            1|           A Dijiang|     1|  24| 180.0|80.0|      199|           1|     NA|            1|      39|        1|\n",
      "|            2|            A Lamusi|     1|  23| 170.0|60.0|      199|           2|     NA|            2|      49|        2|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|   0.0| 0.0|      273|           3|     NA|            3|       7|        3|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|   0.0| 0.0|      278|           4|   Gold|            4|       2|        4|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           5|     NA|            5|      36|        5|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           6|     NA|            5|      36|        6|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           7|     NA|            5|      38|        5|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           8|     NA|            5|      38|        6|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           9|     NA|            5|      40|        5|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|          10|     NA|            5|      40|        6|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          11|     NA|            6|      38|        7|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          12|     NA|            6|      38|        8|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          13|     NA|            6|      38|        9|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          14|     NA|            6|      38|       10|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          15|     NA|            6|      40|        7|\n",
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Primer join\n",
    "deportis_df.join(resultados_df, deportis_df.deportista_id == resultados_df.deportista_id, \"left\")\\\n",
    ".show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53a9609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+--------+-------------+----+---------+-----------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|resultado_id|medalla|deportista_id|juego_id|evento_id|juego_id| nombre_juego|anio|temporada|     ciudad|\n",
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+--------+-------------+----+---------+-----------+\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           9|     NA|            5|      40|        5|      40|1994 Invierno|1994| Invierno|Lillehammer|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|          10|     NA|            5|      40|        6|      40|1994 Invierno|1994| Invierno|Lillehammer|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          15|     NA|            6|      40|        7|      40|1994 Invierno|1994| Invierno|Lillehammer|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          16|     NA|            6|      40|       11|      40|1994 Invierno|1994| Invierno|Lillehammer|\n",
      "|            1|           A Dijiang|     1|  24| 180.0|80.0|      199|           1|     NA|            1|      39|        1|      39|  1992 Verano|1992|   Verano|  Barcelona|\n",
      "|            2|            A Lamusi|     1|  23| 170.0|60.0|      199|           2|     NA|            2|      49|        2|      49|  2012 Verano|2012|   Verano|     London|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|   0.0| 0.0|      273|           3|     NA|            3|       7|        3|       7|  1920 Verano|1920|   Verano|  Antwerpen|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           7|     NA|            5|      38|        5|      38|1992 Invierno|1992| Invierno|Albertville|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           8|     NA|            5|      38|        6|      38|1992 Invierno|1992| Invierno|Albertville|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          11|     NA|            6|      38|        7|      38|1992 Invierno|1992| Invierno|Albertville|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          12|     NA|            6|      38|        8|      38|1992 Invierno|1992| Invierno|Albertville|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          13|     NA|            6|      38|        9|      38|1992 Invierno|1992| Invierno|Albertville|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          14|     NA|            6|      38|       10|      38|1992 Invierno|1992| Invierno|Albertville|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|   0.0| 0.0|      278|           4|   Gold|            4|       2|        4|       2|  1900 Verano|1900|   Verano|      Paris|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           5|     NA|            5|      36|        5|      36|1988 Invierno|1988| Invierno|    Calgary|\n",
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+--------+-------------+----+---------+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Segundo join\n",
    "deportis_df.join(resultados_df, deportis_df.deportista_id == resultados_df.deportista_id, \"left\")\\\n",
    ".join(juegos_df, resultados_df.juego_id == juegos_df.juego_id, \"left\")\\\n",
    ".show(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a541d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:====(1 + 0) / 1][Stage 28:>   (0 + 1) / 1][Stage 29:>   (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+--------+-------------+----+---------+-----------+---------+--------------------+----------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|resultado_id|medalla|deportista_id|juego_id|evento_id|juego_id| nombre_juego|anio|temporada|     ciudad|evento_id|              evento|deporte_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+--------+-------------+----+---------+-----------+---------+--------------------+----------+\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           9|     NA|            5|      40|        5|      40|1994 Invierno|1994| Invierno|Lillehammer|        5|Speed Skating Wom...|         5|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|          10|     NA|            5|      40|        6|      40|1994 Invierno|1994| Invierno|Lillehammer|        6|\"Speed Skating Wo...|         5|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          15|     NA|            6|      40|        7|      40|1994 Invierno|1994| Invierno|Lillehammer|        7|Cross Country Ski...|         6|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          16|     NA|            6|      40|       11|      40|1994 Invierno|1994| Invierno|Lillehammer|       11|Cross Country Ski...|         6|\n",
      "|            1|           A Dijiang|     1|  24| 180.0|80.0|      199|           1|     NA|            1|      39|        1|      39|  1992 Verano|1992|   Verano|  Barcelona|        1|Basketball Men's ...|         1|\n",
      "|            2|            A Lamusi|     1|  23| 170.0|60.0|      199|           2|     NA|            2|      49|        2|      49|  2012 Verano|2012|   Verano|     London|        2|Judo Men's Extra-...|         2|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|   0.0| 0.0|      273|           3|     NA|            3|       7|        3|       7|  1920 Verano|1920|   Verano|  Antwerpen|        3|Football Men's Fo...|         3|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           7|     NA|            5|      38|        5|      38|1992 Invierno|1992| Invierno|Albertville|        5|Speed Skating Wom...|         5|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           8|     NA|            5|      38|        6|      38|1992 Invierno|1992| Invierno|Albertville|        6|\"Speed Skating Wo...|         5|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          11|     NA|            6|      38|        7|      38|1992 Invierno|1992| Invierno|Albertville|        7|Cross Country Ski...|         6|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          12|     NA|            6|      38|        8|      38|1992 Invierno|1992| Invierno|Albertville|        8|Cross Country Ski...|         6|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          13|     NA|            6|      38|        9|      38|1992 Invierno|1992| Invierno|Albertville|        9|Cross Country Ski...|         6|\n",
      "|            6|     Per Knut Aaland|     1|  31| 188.0|75.0|     1096|          14|     NA|            6|      38|       10|      38|1992 Invierno|1992| Invierno|Albertville|       10|Cross Country Ski...|         6|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|   0.0| 0.0|      278|           4|   Gold|            4|       2|        4|       2|  1900 Verano|1900|   Verano|      Paris|        4|Tug-Of-War Men's ...|         4|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|      705|           5|     NA|            5|      36|        5|      36|1988 Invierno|1988| Invierno|    Calgary|        5|Speed Skating Wom...|         5|\n",
      "+-------------+--------------------+------+----+------+----+---------+------------+-------+-------------+--------+---------+--------+-------------+----+---------+-----------+---------+--------------------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Tercer join\n",
    "deportis_df.join(resultados_df, deportis_df.deportista_id == resultados_df.deportista_id, \"left\")\\\n",
    ".join(juegos_df, resultados_df.juego_id == juegos_df.juego_id, \"left\")\\\n",
    ".join(evento_df, resultados_df.evento_id == evento_df.evento_id, \"left\")\\\n",
    ".show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5d0e016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------+----+--------------------+\n",
      "|              nombre|edad_jugador|medalla| año|   nombre_disciplina|\n",
      "+--------------------+------------+-------+----+--------------------+\n",
      "|Christine Jacoba ...|          21|     NA|1994|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|          21|     NA|1994|\"Speed Skating Wo...|\n",
      "|     Per Knut Aaland|          31|     NA|1994|Cross Country Ski...|\n",
      "|     Per Knut Aaland|          31|     NA|1994|Cross Country Ski...|\n",
      "|           A Dijiang|          24|     NA|1992|Basketball Men's ...|\n",
      "|            A Lamusi|          23|     NA|2012|Judo Men's Extra-...|\n",
      "| Gunnar Nielsen Aaby|          24|     NA|1920|Football Men's Fo...|\n",
      "|Christine Jacoba ...|          21|     NA|1992|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|          21|     NA|1992|\"Speed Skating Wo...|\n",
      "|     Per Knut Aaland|          31|     NA|1992|Cross Country Ski...|\n",
      "|     Per Knut Aaland|          31|     NA|1992|Cross Country Ski...|\n",
      "|     Per Knut Aaland|          31|     NA|1992|Cross Country Ski...|\n",
      "|     Per Knut Aaland|          31|     NA|1992|Cross Country Ski...|\n",
      "|Edgar Lindenau Aabye|          34|   Gold|1900|Tug-Of-War Men's ...|\n",
      "|Christine Jacoba ...|          21|     NA|1988|Speed Skating Wom...|\n",
      "+--------------------+------------+-------+----+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Realizando el select\n",
    "deportis_df.join(resultados_df, deportis_df.deportista_id == resultados_df.deportista_id, \"left\")\\\n",
    ".join(juegos_df, resultados_df.juego_id == juegos_df.juego_id, \"left\")\\\n",
    ".join(evento_df, resultados_df.evento_id == evento_df.evento_id, \"left\")\\\n",
    ".select(deportis_df.nombre, f.col(\"edad\").alias(\"edad_jugador\"), \"medalla\", \n",
    "        f.col(\"anio\").alias(\"año\"), evento_df.evento.alias(\"nombre_disciplina\"))\\\n",
    ".show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60400d",
   "metadata": {},
   "source": [
    "### join para armar df con \"medallas ganadoras\" + \"paises\" + \"equipos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b66f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+\n",
      "|pais_id|              equipo|sigla|\n",
      "+-------+--------------------+-----+\n",
      "|      1|         30. Februar|  AUT|\n",
      "|      2|A North American ...|  MEX|\n",
      "|      3|           Acipactli|  MEX|\n",
      "|      4|             Acturus|  ARG|\n",
      "|      5|         Afghanistan|  AFG|\n",
      "+-------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deportis_df.show(5)\n",
    "#deporte_df.show(5)\n",
    "#deportistaError_df.show(5)\n",
    "#evento_df.show(5)\n",
    "#juegos_df.show(5)\n",
    "paises_df.show(5)\n",
    "#esultados_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9afd1b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- resultado_id: integer (nullable = false)\n",
      " |-- medalla: string (nullable = false)\n",
      " |-- deportista_id: integer (nullable = false)\n",
      " |-- juego_id: integer (nullable = false)\n",
      " |-- evento_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d87e76fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pais_id: integer (nullable = false)\n",
      " |-- equipo: string (nullable = false)\n",
      " |-- sigla: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paises_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "565dd53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deportista_id: integer (nullable = false)\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- genero: integer (nullable = false)\n",
      " |-- edad: integer (nullable = false)\n",
      " |-- altura: float (nullable = false)\n",
      " |-- peso: float (nullable = false)\n",
      " |-- equipo_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportis_df.printSchema()\n",
    "#Para que las relaciones sean más claras, vamos a renombrar \"equipo_id\" por \"pais_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abb17f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3896a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 5 ./curso_spark_git_clone/curso-apache-spark-platzi/files/deportista.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "884474a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+-------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|pais_id|\n",
      "+-------------+--------------------+------+----+------+----+-------+\n",
      "|            1|           A Dijiang|     1|  24| 180.0|80.0|    199|\n",
      "|            2|            A Lamusi|     1|  23| 170.0|60.0|    199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|   0.0| 0.0|    273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|   0.0| 0.0|    278|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|    705|\n",
      "+-------------+--------------------+------+----+------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|           4|   Gold|            4|       2|        4|\n",
      "|          38| Bronze|           15|       7|       19|\n",
      "|          39| Bronze|           15|       7|       20|\n",
      "|          41| Bronze|           16|      50|       14|\n",
      "|          42| Bronze|           17|      17|       21|\n",
      "|          43|   Gold|           17|      17|       22|\n",
      "|          45|   Gold|           17|      17|       24|\n",
      "|          49|   Gold|           17|      17|       28|\n",
      "|          51| Bronze|           17|      19|       22|\n",
      "|          61|   Gold|           20|      38|       32|\n",
      "|          62| Bronze|           20|      38|       33|\n",
      "|          64| Silver|           20|      40|       31|\n",
      "|          65| Bronze|           20|      40|       32|\n",
      "|          68| Silver|           20|      40|       35|\n",
      "|          74|   Gold|           20|      44|       32|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Para deportis_df renombramos su columna \"equipo_id\" por \"pais_id\"\n",
    "deportis_df_renamed = deportis_df.withColumnRenamed(\"equipo_id\", \"pais_id\")\n",
    "deportis_df_renamed.show(5)\n",
    "\n",
    "#Pra \"resultados_df\" quitamos los resultados sin medallas \"NA\"\n",
    "resultados_sin_NA_df = resultados_df.filter(f.col(\"medalla\") != 'NA')\n",
    "resultados_sin_NA_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bd21620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 159:========>        (1 + 1) / 2][Stage 160:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+\n",
      "|medalla|sigla|  equipo|\n",
      "+-------+-----+--------+\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "| Silver|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "| Silver|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "+-------+-----+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "resultados_sin_NA_df.join(deportis_df_renamed, resultados_sin_NA_df.deportista_id==deportis_df_renamed.deportista_id, \"left\")\\\n",
    ".join(paises_df, deportis_df_renamed.pais_id ==  paises_df.pais_id, \"left\")\\\n",
    ".select(\"medalla\", \"sigla\", \"equipo\")\\\n",
    ".sort(f.col(\"sigla\").desc())\\\n",
    ".show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad09103",
   "metadata": {},
   "source": [
    "### <a name=\"mark_18\"></a> Funciones de agrupación\n",
    "\n",
    "### [Index](#index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac2774f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+-------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|pais_id|\n",
      "+-------------+--------------------+------+----+------+----+-------+\n",
      "|            1|           A Dijiang|     1|  24| 180.0|80.0|    199|\n",
      "|            2|            A Lamusi|     1|  23| 170.0|60.0|    199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|   0.0| 0.0|    273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|   0.0| 0.0|    278|\n",
      "|            5|Christine Jacoba ...|     2|  21| 185.0|82.0|    705|\n",
      "+-------------+--------------------+------+----+------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|           1|     NA|            1|      39|        1|\n",
      "|           2|     NA|            2|      49|        2|\n",
      "|           3|     NA|            3|       7|        3|\n",
      "|           4|   Gold|            4|       2|        4|\n",
      "|           5|     NA|            5|      36|        5|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 169:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deportis_df_renamed.show(5)\n",
    "#deporte_df.show(5)\n",
    "#deportistaError_df.show(5)\n",
    "#evento_df.show(5)\n",
    "#juegos_df.show(5)\n",
    "#paises_df.show(5)\n",
    "resultados_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f7947dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 266:>                (0 + 1) / 1][Stage 267:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+--------------------+-----------------+--------------------+\n",
      "|sigla|anio|medalla| nombre_subdiciplina|nombre_disciplina|              nombre|\n",
      "+-----+----+-------+--------------------+-----------------+--------------------+\n",
      "|  CHN|1992|     NA|Basketball Men's ...|       Basketball|           A Dijiang|\n",
      "|  CHN|2012|     NA|Judo Men's Extra-...|             Judo|            A Lamusi|\n",
      "|  DEN|1920|     NA|Football Men's Fo...|         Football| Gunnar Nielsen Aaby|\n",
      "|  SWE|1900|   Gold|Tug-Of-War Men's ...|       Tug-Of-War|Edgar Lindenau Aabye|\n",
      "|  NED|1988|     NA|Speed Skating Wom...|    Speed Skating|Christine Jacoba ...|\n",
      "+-----+----+-------+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "medallistas_df = deportis_df_renamed\\\n",
    ".join(resultados_df, deportis_df_renamed.deportista_id == resultados_df.deportista_id, \"left\")\\\n",
    ".join(juegos_df, resultados_df.juego_id == juegos_df.juego_id, \"left\")\\\n",
    ".join(paises_df, deportis_df_renamed.pais_id == paises_df.pais_id, \"left\")\\\n",
    ".join(evento_df, resultados_df.evento_id == evento_df.evento_id, \"left\")\\\n",
    ".join(deporte_df, evento_df.deporte_id ==  deporte_df.deporte_id, \"left\")\\\n",
    ".select(\"sigla\", \"anio\", \"medalla\", \n",
    "        evento_df.evento.alias(\"nombre_subdiciplina\"),\n",
    "        deporte_df.deporte.alias(\"nombre_disciplina\"),\n",
    "        deportis_df.nombre\n",
    "       )\n",
    "\n",
    "medallistas_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ce5f7",
   "metadata": {},
   "source": [
    "### <a name=\"mark_19\"></a> groupBy()\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f22e23dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+-----+\n",
      "|sigla|anio| nombre_subdiciplina|count|\n",
      "+-----+----+--------------------+-----+\n",
      "|  AFG|2008|Taekwondo Men's F...|    1|\n",
      "|  AFG|2012|Taekwondo Men's F...|    1|\n",
      "|  AHO|1988|Sailing Mixed Win...|    1|\n",
      "|  ALB|2000|Athletics Women's...|    1|\n",
      "|  ALB|2004|Athletics Women's...|    1|\n",
      "|  ALB|2016|Judo Women's Half...|    1|\n",
      "|  ALG|1984|Boxing Men's Ligh...|    1|\n",
      "|  ALG|1984|Boxing Men's Midd...|    1|\n",
      "|  ALG|1992|\"Athletics Women'...|    1|\n",
      "|  ALG|1992|Boxing Men's Feat...|    1|\n",
      "|  ALG|1996|\"Athletics Men's ...|    1|\n",
      "|  ALG|1996|Boxing Men's Ligh...|    1|\n",
      "|  ALG|1996|Boxing Men's Midd...|    1|\n",
      "|  ALG|2000|\"Athletics Men's ...|    1|\n",
      "|  ALG|2000|\"Athletics Women'...|    1|\n",
      "|  ALG|2000|Athletics Men's 8...|    1|\n",
      "|  ALG|2000|Athletics Men's H...|    1|\n",
      "|  ALG|2000|Boxing Men's Ligh...|    1|\n",
      "|  ALG|2008|Judo Men's Middle...|    1|\n",
      "|  ALG|2008|Judo Women's Half...|    1|\n",
      "|  ALG|2012|\"Athletics Men's ...|    1|\n",
      "|  ALG|2016|\"Athletics Men's ...|    1|\n",
      "|  ALG|2016|Athletics Men's 8...|    1|\n",
      "|  ANZ|1908|\"Athletics Men's ...|    1|\n",
      "|  ANZ|1908|\"Swimming Men's 1...|    1|\n",
      "+-----+----+--------------------+-----+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#descartamos los que no tienen medallas y agrupamos\n",
    "medallistas_df_01=medallistas_df\\\n",
    ".filter(medallistas_df.medalla != 'NA')\\\n",
    ".groupBy(\"sigla\", \"anio\", \"nombre_subdiciplina\")\\\n",
    ".count()\\\n",
    ".sort(\"sigla\", \"anio\", \"nombre_subdiciplina\", \"count\")\n",
    "\n",
    "medallistas_df_01.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f69b4",
   "metadata": {},
   "source": [
    "### <a name=\"mark_20\"></a> ejemplo con withColumn(\"nombre_columna\", \"operacion_con_cast\")\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c9e9551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sigla: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- nombre_subdiciplina: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- sigla: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- nombre_subdiciplina: string (nullable = true)\n",
      " |-- count: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medallistas_df_01.printSchema()\n",
    "\n",
    "#Casteando columna \"count\" a \"int\"\n",
    "medallistas_df_02=medallistas_df_01.withColumn(\"count\", medallistas_df_01[\"count\"].cast(IntegerType()))\n",
    "\n",
    "medallistas_df_02.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348f0ff",
   "metadata": {},
   "source": [
    "### <a name=\"mark_21\"></a> .agg() la forma correcta de hacer agregaciones\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- Ahora vamos a realizar una agrupación por \"sigla\" y por \"anio\" de medallista_df_01\n",
    "\n",
    "- Documentacion: https://sparkbyexamples.com/pyspark/pyspark-groupby-agg-aggregate-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "843a438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------------+------------------+\n",
      "|sigla|anio|medallas_totales| promedio_medallas|\n",
      "+-----+----+----------------+------------------+\n",
      "|  URS|1980|             496| 2.883720930232558|\n",
      "|  USA|1904|             398|4.4222222222222225|\n",
      "|  URS|1988|             366| 2.772727272727273|\n",
      "|  USA|1984|             361|2.5069444444444446|\n",
      "|  GBR|1908|             354| 4.597402597402597|\n",
      "|  URS|1976|             342|          2.671875|\n",
      "|  USA|2008|             318|3.4565217391304346|\n",
      "|  GDR|1980|             303| 2.443548387096774|\n",
      "|  USA|2016|             263| 2.481132075471698|\n",
      "|  USA|2004|             262| 2.977272727272727|\n",
      "|  USA|1996|             259|2.9101123595505616|\n",
      "|  URS|1972|             259|2.5145631067961167|\n",
      "|  FRA|1900|             250| 4.310344827586207|\n",
      "|  USA|2012|             248|2.7252747252747254|\n",
      "|  USA|2000|             242|             3.025|\n",
      "+-----+----+----------------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Recordar que \"sum\", \"avg\", \"count\", \"max\", etc provenientes de \"function\" fueron importadas \"as f\"\n",
    "medallistas_df_03=medallistas_df_01.groupBy('sigla', 'anio')\\\n",
    "    .agg(f.sum('count').alias('medallas_totales'),\\\n",
    "         f.avg('count').alias('promedio_medallas'))\n",
    "medallistas_df_03.orderBy(f.desc(\"medallas_totales\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb52bbe",
   "metadata": {},
   "source": [
    "### <a name=\"mark_22\"></a> SQL\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b743325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+--------+---------+\n",
      "|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|           1|     NA|            1|      39|        1|\n",
      "|           2|     NA|            2|      49|        2|\n",
      "|           3|     NA|            3|       7|        3|\n",
      "|           4|   Gold|            4|       2|        4|\n",
      "|           5|     NA|            5|      36|        5|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 823:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#deportis_df_renamed.show(5)\n",
    "#deporte_df.show(5)\n",
    "#deportistaError_df.show(5)\n",
    "#evento_df.show(5)\n",
    "#juegos_df.show(5)\n",
    "#paises_df.show(5)\n",
    "resultados_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ddb37a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultados_df.createOrReplaceTempView(\"resultados_table\")\n",
    "\n",
    "deportis_df_renamed.createOrReplaceTempView(\"deportis_table\")\n",
    "\n",
    "paises_df.createOrReplaceTempView(\"paises_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0b6a8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b2310cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay que instanciar SQLContext con el contexto que ya estamos usando \"sc_04\"\n",
    "sqlContext = SQLContext(sc_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "69fd5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+--------+---------+\n",
      "|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|           1|     NA|            1|      39|        1|\n",
      "|           2|     NA|            2|      49|        2|\n",
      "|           3|     NA|            3|       7|        3|\n",
      "|           4|   Gold|            4|       2|        4|\n",
      "|           5|     NA|            5|      36|        5|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "SELECT * FROM resultados_table\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repetimos el mismo join pero en SQL\n",
    "\"\"\"\n",
    "resultados_sin_NA_df.join(deportis_df_renamed, resultados_sin_NA_df.deportista_id==deportis_df_renamed.deportista_id, \"left\")\\\n",
    ".join(paises_df, deportis_df_renamed.pais_id ==  paises_df.pais_id, \"left\")\\\n",
    ".select(\"medalla\", \"sigla\", \"equipo\")\\\n",
    ".sort(f.col(\"sigla\").desc())\\\n",
    ".show(15)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f87bd5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 883:========>        (1 + 1) / 2][Stage 884:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+\n",
      "|medalla|sigla|  equipo|\n",
      "+-------+-----+--------+\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "|   Gold|  ZIM|Zimbabwe|\n",
      "+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "SELECT medalla, sigla, equipo FROM resultados_table rt\n",
    "LEFT JOIN deportis_table dt\n",
    "ON rt.deportista_id = dt.deportista_id\n",
    "LEFT JOIN paises_table pt\n",
    "ON dt.pais_id = pt.pais_id\n",
    "WHERE medalla != \"NA\"\n",
    "ORDER BY sigla DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d13b6fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.145:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>new_rdd</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=new_rdd>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed5db9",
   "metadata": {},
   "source": [
    "### <a name=\"mark_23\"></a> UDF \"User-defined function\"\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- Las funciones definidas por el usuario o UDF, por sus siglas en inglés, son una funcionalidad agregada en Spark para definir funciones basadas en columnas las cuales permiten extender las capacidades de Spark al momento de transformar el set de datos.\n",
    "\n",
    "- Este tipo de implementaciones son convenientes cuando tenemos un desarrollo extenso donde hemos identificado la periodicidad de tareas repetitivas como suele ser en pasos de limpieza de datos, transformación o renombrado dinámico de columnas.\n",
    "\n",
    "- Por lo anterior es común encontrar en un proyecto de Spark una librería independiente donde existen todas estas funciones agregadas para que los desarrolladores involucrados en el proyecto puedan usarlas a conveniencia.\n",
    "\n",
    "- El uso de UDF no implica que las funciones que podemos crear nativamente con Python, Scala, R o Java no sean útiles. Una UDF tiene el objetivo de ofrecer un estándar interno en el proyecto que nos encontremos realizando. Además, en caso de ser necesario, una UDF puede ser modificada con ayuda de decoradores para que sea más extensible en diversos escenarios a los cuales nos podemos enfrentar.\n",
    "\n",
    "- Otro motivo para usar UDF es que en el módulo de Spark MLlib, la librería nativa de Spark para operaciones de Machine Learning, las UDF juegan un papel vital al momento de hacer transformaciones. Por lo cual tener un uso familiar de estas ampliará considerablemente la curva de aprendizaje de Spark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d040e546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14520\r\n",
      "drwxr-xr-x 2 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  9 09:38 .\r\n",
      "drwxr-xr-x 5 compu_dell_ubuntu_01 compu_dell_ubuntu_01    4096 Oct  7 13:11 ..\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01     946 Oct  7 13:11 deporte.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2764536 Oct  7 13:11 deportista.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2807091 Oct  7 13:11 deportista2.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 2776782 Oct  7 13:11 deportistaError.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   32942 Oct 24 13:40 evento.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    1978 Oct  7 13:11 juegos.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01  255853 Oct  7 13:11 modelo_relacional.jpg\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   23606 Oct  7 13:11 paises.csv\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01 6172768 Oct 24 14:13 resultados.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "961b1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc_04)\n",
    "deportistaError_df = sqlContext.read.csv(path+\"deportistaError.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd98629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24|   180|80.0|      199|\n",
      "|            2|            A Lamusi|     1|  23|   170|60.0|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|  null|null|      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|  null|null|      278|\n",
      "|            5|Christine Jacoba ...|     2|  21|   185|82.0|      705|\n",
      "|            6|     Per Knut Aaland|     1|  31|   188|75.0|     1096|\n",
      "|            7|        John Aalberg|     1|  31|   183|72.0|     1096|\n",
      "|            8|\"Cornelia \"\"Cor\"\"...|     2|  18|   168|null|      705|\n",
      "|            9|    Antti Sami Aalto|     1|  26|   186|96.0|      350|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaError_df.show(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2426e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8faf9",
   "metadata": {},
   "source": [
    "### Creando la Función que utilizaremos como UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e84d3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos valores \"null\" a \"0\"\n",
    "def conversion_nulls(valor):\n",
    "    return 0 if valor == None else valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af754ef2",
   "metadata": {},
   "source": [
    "### Creamos la función que va ser tratada como UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf66ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_a_cero_udf = udf(lambda z: conversion_nulls(z), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a10f6",
   "metadata": {},
   "source": [
    "### Damos de alta \"conversion_a_cero_udf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e4ec068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/26 10:11:20 WARN SimpleFunctionRegistry: The function conversion_a_cero_udf replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#El primer parámetro es el nombre que Spark le va a dar.\n",
    "#El segundo parámetro es la función que va registrar.\n",
    "sqlContext.udf.register(\"conversion_a_cero_udf\", conversion_a_cero_udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad0e84",
   "metadata": {},
   "source": [
    "### Aplicando la UDF\n",
    "\n",
    "- Cual sería el benficio de estas UDF.\n",
    "    - Trabajar nativamente.\n",
    "    - Invocarlas y utilizarlas con sql sin problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c15615a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|altura_udf|\n",
      "+----------+\n",
      "|       180|\n",
      "|       170|\n",
      "|         0|\n",
      "|         0|\n",
      "|       185|\n",
      "|       188|\n",
      "|       183|\n",
      "|       168|\n",
      "|       186|\n",
      "|         0|\n",
      "|       182|\n",
      "|       172|\n",
      "|       159|\n",
      "|       171|\n",
      "|         0|\n",
      "|       184|\n",
      "|       175|\n",
      "|       189|\n",
      "|         0|\n",
      "|       176|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportistaError_df.select(conversion_a_cero_udf(\"altura\").alias(\"altura_udf\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fee140",
   "metadata": {},
   "source": [
    "### <a name=\"mark_24\"></a> Comprendiendo la persistencia y particionado\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "- Como se ha descrito en clases pasadas, los RDD son la capa de abstracción primaria para poder interactuar con los datos que viven en nuestro ambiente de Spark. Aunque estos puedan ser enmascarados con un esquema dotándolos de las facultades propias de los DataFrames, la información de fondo sigue operando como RDD.\n",
    "\n",
    "- Por lo tanto, la información, como indica el nombre de los RDD, se maneja de forma distribuida a lo largo del clúster, facilitando las operaciones que se van a ejecutar, ya que segmentos de información pueden encontrarse en diferentes ejecutores reduciendo el tiempo necesario para acceder a la información y poder así realizar los cálculos necesarios.\n",
    "\n",
    "- Cuando un RDD o Dataframe es creado, según las especificaciones que se indiquen a la aplicación de Spark, creará un esquema de particionado básico, el cual distribuirá los datos a lo largo del clúster. Siendo así que al momento de ejecutar una acción, esta se ejecutará entre los diversos fragmentos de información que existan para poder así realizar de la forma más rápida las operaciones. Es por eso que un correcto esquema de particionado es clave para poder tener aplicaciones rápidas y precisas que además consuman pocos recursos de red.\n",
    "\n",
    "- Otra de las tareas fundamentales es la replicación de componentes y sus fragmentos, ya que al aumentar la disponibilidad de estos podremos asegurar una tolerancia a fallos, mientras más se replique un valor es más probable que no se pierda si existe un fallo de red o energía, además de permitir una disponibilidad casi inmediata del archivo buscado.\n",
    "\n",
    "- La partición y replicación son elementos que deben ser analizados según el tipo de negocio o requerimientos que se tengan en el desarrollo que se encuentre en progreso, por lo cual la cantidad de datos replicados o granularidad de datos existentes en los fragmentos dependerá en función de las reglas de negocio.\n",
    "\n",
    "![](img_21.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_22.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_23.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_24.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "![](img_25.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8713fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d0f92",
   "metadata": {},
   "source": [
    "### <a name=\"mark_25\"></a> Preguntamos si \"deportistaError_df\" está en Cache\n",
    "\n",
    "### [Index](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "305d6c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError_df.is_cached\n",
    "\n",
    "\"\"\"\n",
    "Como se puede ver la respuesta es \"false\", con lo cual cada vez que lo llame Spark lo tiene que\n",
    "reconstruir.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3d797b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[91] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError_df.rdd.cache()\n",
    "\n",
    "\"\"\"\n",
    "Aquí utilizamos las funciones primitivas \"rdd\" aplicadas al df y su guardado en cache, y para\n",
    "este ejemplo obtenemos la siguiente salida.\n",
    "\n",
    "\"MapPartitionsRDD[91] at javaToPython at NativeMethodAccessorImpl.java:0\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db219482",
   "metadata": {},
   "source": [
    "### <a name=\"mark_26\"></a> Que tipo de persistencia tiene un DF? \"getStorageLevel()\"\n",
    "### [Index](#index) \n",
    "\n",
    "- Documentación https://spark.apache.org/docs/2.4.6/api/python/pyspark.html#pyspark.StorageLevel\n",
    "\n",
    "\n",
    "![](img_26.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "154604b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError_df.rdd.getStorageLevel()\n",
    "\n",
    "\"\"\"\n",
    "Para este ejemplo vemos la siguiente salida.\n",
    "\n",
    "StorageLevel(False, True, False, False, 1)\n",
    "\n",
    "lo que nos indica un \"MEMORY_ONLY\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bab9c",
   "metadata": {},
   "source": [
    "### <a name=\"mark_27\"></a> unpersist()\n",
    "### [Index](#index) \n",
    "\n",
    "- Si deseo cambiar el tipo de \"StorageLevel\" primero tengo que dejar de persistir los datos que estan en cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df52c31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[91] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError_df.rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f1e94",
   "metadata": {},
   "source": [
    "### <a name=\"mark_28\"></a> Cambiando el tipo de \"StorageLevel\"\n",
    "### [Index](#index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaError_df.rdd.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "\n",
    "\"\"\"\n",
    "DF persistido para que pueda utilizar desde Memoria o Disk, según sea la necesidad de Spark,\n",
    "y replicado 2 veces.\n",
    "\n",
    "Nota: Si hubieramos creado un particionada, previo, este particionado sería el que va a ser \n",
    "replicado 2 veces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d839968",
   "metadata": {},
   "source": [
    "### <a name=\"mark_29\"></a> Creando mi propio \"StorageLevel\", persistencia de datos.\n",
    "### [Index](#index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a77ad26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSe usa el nombre \"MEMORY_AND_DISK_3\" solo como una buena práctica, pero podría ser cualquier\\notro, también elegimos 3 replicaciones que persisten los datos en 3 \"lugares\" diferentes para\\nno perderlos en ocación de algún fallo.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creación\n",
    "StorageLevel.MEMORY_AND_DISK_3 = StorageLevel(True,True,False,False,3)\n",
    "\n",
    "\"\"\"\n",
    "Se usa el nombre \"MEMORY_AND_DISK_3\" solo como una buena práctica, pero podría ser cualquier\n",
    "otro, también elegimos 3 replicaciones que persisten los datos en 3 \"lugares\" diferentes para\n",
    "no perderlos en ocación de algún fallo.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "433c17ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[91] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicación del nuevo StorageLevel.\n",
    "\n",
    "deportistaError_df.rdd.persist(StorageLevel.MEMORY_AND_DISK_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a609a706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, False, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError_df.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9eeb90a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[91] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError_df.rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cabb035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportistaError_df.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2ae86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_04.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23798e",
   "metadata": {},
   "source": [
    "### <a name=\"mark_30\"></a> Particionando datos, RDD o DF.\n",
    "### [Index](#index) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c63a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "787b857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"particionado\").master(\"local[5]\").getOrCreate()\n",
    "\n",
    "# \"local[5]\" indica por defecto hacer un particionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e90daf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creamos un DF\n",
    "df = spark.range(0,20)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61c802",
   "metadata": {},
   "source": [
    "### <a name=\"mark_31\"></a> Ver cantidad de particiones getNumPartitions()\n",
    "### [Index](#index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "070af9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "#Por defecto construyó 5 particiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1c39d",
   "metadata": {},
   "source": [
    "### Un ejemplo similar al anterior pero utilizando parámetros manuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2b43901",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_01 = spark.sparkContext.parallelize((0,20),12)\n",
    "\n",
    "\"\"\"\n",
    "Como SparkSession contiene también los sparkContext, puedo crear ese rdd, con 12 particiones.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "505c08d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 20]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_01.take(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "761ee0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_01.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165d645",
   "metadata": {},
   "source": [
    "### Segundo ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7256005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos el rdd, con 12 particiones\n",
    "\n",
    "deporte_rdd = spark\\\n",
    "    .sparkContext\\\n",
    "    .textFile(path+\"deporte.csv\", 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c7b86de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deporte_rdd.getNumPartitions() #el valor es una partición más de la seteada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f563a57",
   "metadata": {},
   "source": [
    "### <a name=\"mark_32\"></a> Guardando los rdd o df con saveAsTextFile()\n",
    "### [Index](#index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34d1323c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./curso_spark_git_clone/curso-apache-spark-platzi/files/'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='./curso_spark_git_clone/curso-apache-spark-platzi/files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbdec834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:=================================>                        (8 + 6) / 14]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deporte_rdd.saveAsTextFile(path+'deporte_partitions')\n",
    "\"\"\"\n",
    "Guardamos \"deporte_rdd\" dentro de una nueva carpeta \"deporte_partitions\", lo cual como vemos al\n",
    "inspeccionar la carpeta todas las particiones generadas y archivo _SUCCESS, este archivo siempre\n",
    "debe venir vacios, si no es así tal vez halla habido errores.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d39511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 120\r\n",
      "drwxr-xr-x 2 compu_dell_ubuntu_01 compu_dell_ubuntu_01 4096 Oct 26 12:13 .\r\n",
      "drwxr-xr-x 3 compu_dell_ubuntu_01 compu_dell_ubuntu_01 4096 Oct 26 12:13 ..\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    8 Oct 26 12:13 ._SUCCESS.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00000.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00001.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00002.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00003.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00004.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00005.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00006.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00007.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00008.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00009.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00010.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00011.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   12 Oct 26 12:13 .part-00012.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    8 Oct 26 12:13 .part-00013.crc\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    0 Oct 26 12:13 _SUCCESS\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   79 Oct 26 12:13 part-00000\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   72 Oct 26 12:13 part-00001\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   74 Oct 26 12:13 part-00002\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   64 Oct 26 12:13 part-00003\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   73 Oct 26 12:13 part-00004\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   87 Oct 26 12:13 part-00005\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   63 Oct 26 12:13 part-00006\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   72 Oct 26 12:13 part-00007\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   75 Oct 26 12:13 part-00008\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   75 Oct 26 12:13 part-00009\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   62 Oct 26 12:13 part-00010\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   70 Oct 26 12:13 part-00011\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01   80 Oct 26 12:13 part-00012\r\n",
      "-rw-r--r-- 1 compu_dell_ubuntu_01 compu_dell_ubuntu_01    0 Oct 26 12:13 part-00013\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lac ./curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375099a0",
   "metadata": {},
   "source": [
    "### Viendo las particiones.\n",
    "\n",
    "- Acá se puede ver como fue particionado el rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "02590755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deporte_id,deporte\r\n",
      "1,Basketball\r\n",
      "2,Judo\r\n",
      "3,Football\r\n",
      "4,Tug-Of-War\r\n",
      "5,Speed Skating\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 7 ./curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f20d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,Cross Country Skiing\r\n",
      "7,Athletics\r\n",
      "8,Ice Hockey\r\n",
      "9,Swimming\r\n",
      "10,Badminton\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 7 ./curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcaa340",
   "metadata": {},
   "source": [
    "### <a name=\"mark_33\"></a> Reconstruyendo rdd desde las particiones wholeTextFiles() Muchos pasos inecesarios.\n",
    "### [Index](#index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b436ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComo vemos la salida no es muy funcional, por lo cual de\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos el rdd cargando todos las particiones.\n",
    "\n",
    "deporte_partitions_path = './curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/'\n",
    "\n",
    "all_partitions_rdd = spark.sparkContext.wholeTextFiles(deporte_partitions_path+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4704917c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00000',\n",
       "  'deporte_id,deporte\\n1,Basketball\\n2,Judo\\n3,Football\\n4,Tug-Of-War\\n5,Speed Skating\\n'),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00001',\n",
       "  '6,Cross Country Skiing\\n7,Athletics\\n8,Ice Hockey\\n9,Swimming\\n10,Badminton\\n'),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00002',\n",
       "  '11,Sailing\\n12,Biathlon\\n13,Gymnastics\\n14,Art Competitions\\n15,Alpine Skiing\\n'),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00003',\n",
       "  '16,Handball\\n17,Weightlifting\\n18,Wrestling\\n19,Luge\\n20,Water Polo\\n'),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00004',\n",
       "  '21,Hockey\\n22,Rowing\\n23,Bobsleigh\\n24,Fencing\\n25,Equestrianism\\n26,Shooting\\n')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_partitions_rdd.take(5)\n",
    "\n",
    "\"\"\"\n",
    "Como vemos la salida no es muy funcional, por lo cual de hay que hacer varios pasos extra.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7737ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero creamos una lista con los componentes de rdd.\n",
    "lista = all_partitions_rdd.mapValues(lambda x: x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "431a9c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00000',\n",
       "  ['deporte_id,deporte',\n",
       "   '1,Basketball',\n",
       "   '2,Judo',\n",
       "   '3,Football',\n",
       "   '4,Tug-Of-War',\n",
       "   '5,Speed',\n",
       "   'Skating']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00001',\n",
       "  ['6,Cross',\n",
       "   'Country',\n",
       "   'Skiing',\n",
       "   '7,Athletics',\n",
       "   '8,Ice',\n",
       "   'Hockey',\n",
       "   '9,Swimming',\n",
       "   '10,Badminton']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00002',\n",
       "  ['11,Sailing',\n",
       "   '12,Biathlon',\n",
       "   '13,Gymnastics',\n",
       "   '14,Art',\n",
       "   'Competitions',\n",
       "   '15,Alpine',\n",
       "   'Skiing']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00003',\n",
       "  ['16,Handball',\n",
       "   '17,Weightlifting',\n",
       "   '18,Wrestling',\n",
       "   '19,Luge',\n",
       "   '20,Water',\n",
       "   'Polo']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00004',\n",
       "  ['21,Hockey',\n",
       "   '22,Rowing',\n",
       "   '23,Bobsleigh',\n",
       "   '24,Fencing',\n",
       "   '25,Equestrianism',\n",
       "   '26,Shooting']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00005',\n",
       "  ['27,Boxing',\n",
       "   '28,Taekwondo',\n",
       "   '29,Cycling',\n",
       "   '30,Diving',\n",
       "   '31,Canoeing',\n",
       "   '32,Tennis',\n",
       "   '33,Modern',\n",
       "   'Pentathlon']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00006',\n",
       "  ['34,Figure',\n",
       "   'Skating',\n",
       "   '35,Golf',\n",
       "   '36,Softball',\n",
       "   '37,Archery',\n",
       "   '38,Volleyball']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00007',\n",
       "  ['39,Synchronized',\n",
       "   'Swimming',\n",
       "   '40,Table',\n",
       "   'Tennis',\n",
       "   '41,Nordic',\n",
       "   'Combined',\n",
       "   '42,Baseball']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00008',\n",
       "  ['43,Rhythmic',\n",
       "   'Gymnastics',\n",
       "   '44,Freestyle',\n",
       "   'Skiing',\n",
       "   '45,Rugby',\n",
       "   'Sevens',\n",
       "   '46,Trampolining']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00009',\n",
       "  ['47,Beach',\n",
       "   'Volleyball',\n",
       "   '48,Triathlon',\n",
       "   '49,Ski',\n",
       "   'Jumping',\n",
       "   '50,Curling',\n",
       "   '51,Snowboarding']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00010',\n",
       "  ['52,Rugby',\n",
       "   '53,Short',\n",
       "   'Track',\n",
       "   'Speed',\n",
       "   'Skating',\n",
       "   '54,Skeleton',\n",
       "   '55,Lacrosse']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00011',\n",
       "  ['56,Polo',\n",
       "   '57,Cricket',\n",
       "   '58,Racquets',\n",
       "   '59,Motorboating',\n",
       "   '60,Military',\n",
       "   'Ski',\n",
       "   'Patrol']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00012',\n",
       "  ['61,Croquet',\n",
       "   '62,Jeu',\n",
       "   'De',\n",
       "   'Paume',\n",
       "   '63,Roque',\n",
       "   '64,Alpinism',\n",
       "   '65,Basque',\n",
       "   'Pelota',\n",
       "   '66,Aeronautics']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00013',\n",
       "  [])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bd1bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_01 = all_partitions_rdd.mapValues(lambda x: x.split(',')).collect()\n",
    "\"\"\"\n",
    "Una prueba usando split(',') muestra que no es lo más conveniente, aparece el salto de carro '\\n'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2453d5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00000',\n",
       "  ['deporte_id',\n",
       "   'deporte\\n1',\n",
       "   'Basketball\\n2',\n",
       "   'Judo\\n3',\n",
       "   'Football\\n4',\n",
       "   'Tug-Of-War\\n5',\n",
       "   'Speed Skating\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00001',\n",
       "  ['6',\n",
       "   'Cross Country Skiing\\n7',\n",
       "   'Athletics\\n8',\n",
       "   'Ice Hockey\\n9',\n",
       "   'Swimming\\n10',\n",
       "   'Badminton\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00002',\n",
       "  ['11',\n",
       "   'Sailing\\n12',\n",
       "   'Biathlon\\n13',\n",
       "   'Gymnastics\\n14',\n",
       "   'Art Competitions\\n15',\n",
       "   'Alpine Skiing\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00003',\n",
       "  ['16',\n",
       "   'Handball\\n17',\n",
       "   'Weightlifting\\n18',\n",
       "   'Wrestling\\n19',\n",
       "   'Luge\\n20',\n",
       "   'Water Polo\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00004',\n",
       "  ['21',\n",
       "   'Hockey\\n22',\n",
       "   'Rowing\\n23',\n",
       "   'Bobsleigh\\n24',\n",
       "   'Fencing\\n25',\n",
       "   'Equestrianism\\n26',\n",
       "   'Shooting\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00005',\n",
       "  ['27',\n",
       "   'Boxing\\n28',\n",
       "   'Taekwondo\\n29',\n",
       "   'Cycling\\n30',\n",
       "   'Diving\\n31',\n",
       "   'Canoeing\\n32',\n",
       "   'Tennis\\n33',\n",
       "   'Modern Pentathlon\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00006',\n",
       "  ['34',\n",
       "   'Figure Skating\\n35',\n",
       "   'Golf\\n36',\n",
       "   'Softball\\n37',\n",
       "   'Archery\\n38',\n",
       "   'Volleyball\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00007',\n",
       "  ['39',\n",
       "   'Synchronized Swimming\\n40',\n",
       "   'Table Tennis\\n41',\n",
       "   'Nordic Combined\\n42',\n",
       "   'Baseball\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00008',\n",
       "  ['43',\n",
       "   'Rhythmic Gymnastics\\n44',\n",
       "   'Freestyle Skiing\\n45',\n",
       "   'Rugby Sevens\\n46',\n",
       "   'Trampolining\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00009',\n",
       "  ['47',\n",
       "   'Beach Volleyball\\n48',\n",
       "   'Triathlon\\n49',\n",
       "   'Ski Jumping\\n50',\n",
       "   'Curling\\n51',\n",
       "   'Snowboarding\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00010',\n",
       "  ['52',\n",
       "   'Rugby\\n53',\n",
       "   'Short Track Speed Skating\\n54',\n",
       "   'Skeleton\\n55',\n",
       "   'Lacrosse\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00011',\n",
       "  ['56',\n",
       "   'Polo\\n57',\n",
       "   'Cricket\\n58',\n",
       "   'Racquets\\n59',\n",
       "   'Motorboating\\n60',\n",
       "   'Military Ski Patrol\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00012',\n",
       "  ['61',\n",
       "   'Croquet\\n62',\n",
       "   'Jeu De Paume\\n63',\n",
       "   'Roque\\n64',\n",
       "   'Alpinism\\n65',\n",
       "   'Basque Pelota\\n66',\n",
       "   'Aeronautics\\n']),\n",
       " ('file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00013',\n",
       "  [''])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a7e12c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00000',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00001',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00002',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00003',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00004',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00005',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00006',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00007',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00008',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00009',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00010',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00011',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00012',\n",
       " 'file:/home/compu_dell_ubuntu_01/platzi/pip_y_entornos_virtuales/py-project/39_Spark_y_Big_Data_Fundamentos/curso_spark_git_clone/curso-apache-spark-platzi/files/deporte_partitions/part-00013']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recuperamos los todos los paths\n",
    "lista_path = [ l[0] for l in lista]\n",
    "lista_path.sort()\n",
    "lista_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea8b310e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEsta parte \"textFile(\\',\\'.join(lista_path),9)\" toma todos los elementos de \"lista_path\" y los une\\ncon una coma, luego genera 9 particiones nuevas.\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos un nuevo rdd utilizando todos los paths recuperados\n",
    "\n",
    "rdd_recuperado = spark\\\n",
    "    .sparkContext\\\n",
    "    .textFile(','.join(lista_path),9).map(lambda line: line.split(','))\n",
    "\"\"\"\n",
    "Esta parte \"textFile(','.join(lista_path),9)\" toma todos los elementos de \"lista_path\" y los une\n",
    "con una coma, luego genera 9 particiones nuevas.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf0b7b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deporte_id', 'deporte'],\n",
       " ['1', 'Basketball'],\n",
       " ['2', 'Judo'],\n",
       " ['3', 'Football'],\n",
       " ['4', 'Tug-Of-War'],\n",
       " ['5', 'Speed Skating'],\n",
       " ['6', 'Cross Country Skiing'],\n",
       " ['7', 'Athletics'],\n",
       " ['8', 'Ice Hockey']]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_recuperado.take(9) #Acá tenemos el rdd reconstruido a partir de las particiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9fb66",
   "metadata": {},
   "source": [
    "### <a name=\"mark_34\"></a> Reconstruyendo rdd desde las particiones textFile() en un solo paso.\n",
    "### [Index](#index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3787164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_partitions_rdd_01 = spark.sparkContext.textFile(deporte_partitions_path+'*')\\\n",
    "    .map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a254620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deporte_id', 'deporte'],\n",
       " ['1', 'Basketball'],\n",
       " ['2', 'Judo'],\n",
       " ['3', 'Football'],\n",
       " ['4', 'Tug-Of-War'],\n",
       " ['5', 'Speed Skating'],\n",
       " ['6', 'Cross Country Skiing'],\n",
       " ['7', 'Athletics'],\n",
       " ['8', 'Ice Hockey']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_partitions_rdd_01.take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e81e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50dc4aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/compu_dell_ubuntu_01/anaconda3/envs/data_trans_env:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "brotlipy                  0.7.0           py311h5eee18b_1002  \n",
      "contourpy                 1.0.5           py311hdb19cb5_0  \n",
      "debugpy                   1.5.1           py311h6a678d5_0  \n",
      "ipykernel                 6.19.2          py311h6410fe4_0  \n",
      "ipython                   8.12.0          py311h06a4308_0  \n",
      "jupyter_client            8.1.0           py311h06a4308_0  \n",
      "jupyter_core              5.3.0           py311h06a4308_0  \n",
      "numpy                     1.24.3          py311h08b1b3b_1  \n",
      "numpy-base                1.24.3          py311hf175353_1  \n",
      "py4j                      0.10.9.7        py311h06a4308_0  \n",
      "pyarrow                   11.0.0          py311hd8e8d9b_1  \n",
      "pycosat                   0.6.4           py311h5eee18b_0  \n",
      "pycparser                 2.21               pyhd3eb1b0_0  \n",
      "pygments                  2.15.1          py311h06a4308_1  \n",
      "pyjanitor                 0.23.1             pyhd8ed1ab_1    conda-forge\n",
      "pyopenssl                 23.0.0          py311h06a4308_0  \n",
      "pyparsing                 3.0.9           py311h06a4308_0  \n",
      "pyprojroot                0.3.0              pyhd8ed1ab_0    conda-forge\n",
      "pyqt                      5.15.7          py311h6a678d5_0  \n",
      "pyqt5-sip                 12.11.0         py311h6a678d5_0  \n",
      "pyreadr                   0.4.7           py311hc24ef6a_2    conda-forge\n",
      "pyrsistent                0.18.0          py311h5eee18b_0  \n",
      "pysocks                   1.7.1           py311h06a4308_0  \n",
      "pyspark                   3.4.1           py311h06a4308_0  \n",
      "python                    3.11.4               h955ad1f_0  \n",
      "python-dateutil           2.8.2              pyhd3eb1b0_0  \n",
      "python-fastjsonschema     2.16.2          py311h06a4308_0  \n",
      "python-slugify            5.0.2              pyhd3eb1b0_0  \n",
      "python_abi                3.11                    2_cp311    conda-forge\n",
      "pytz                      2022.7          py311h06a4308_0  \n",
      "pyyaml                    6.0             py311h5eee18b_1  \n",
      "pyzmq                     25.1.0          py311h6a678d5_0  \n",
      "scipy                     1.10.1          py311h08b1b3b_1  \n",
      "snappy                    1.1.9                h295c915_0  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b239f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "connexion_string = 'mongodb+srv://mendezleonardom:Mongodb_3967@awscluster.bupqt8b.mongodb.net/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccf81bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb+srv://mendezleonardom:Mongodb_3967@awscluster.bupqt8b.mongodb.net/sample_guides.planets\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://mendezleonardom:Mongodb_3967@awscluster.bupqt8b.mongodb.net/sample_guides.planets\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20477dc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o240.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
      "File \u001b[0;32m~/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/data_trans_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o240.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"mongodb\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b6a9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83691ce6",
   "metadata": {},
   "source": [
    "### <a name=\"mark_35\"></a> Data Masking.\n",
    "### [Index](#index) \n",
    "\n",
    "\n",
    "1. Enmascaramiento con Funciones de Spark SQL:\n",
    "\n",
    "- En este ejemplo, la columna sensible se enmascara utilizando la función sha2, que aplica la función de hash SHA-256 a los valores de la columna. El resultado es una versión enmascarada de los datos originales.\n",
    "\n",
    "- En este caso, si has aplicado una función hash como sha2 de Spark SQL para enmascarar los datos, **la recuperación de los datos originales no es directa**. Las funciones hash son unidireccionales, lo que significa que no se pueden revertir. Para recuperar los datos originales, generalmente necesitas comparar el hash de un valor proporcionado con el hash almacenado. Si hay una coincidencia, has encontrado el valor original. Sin embargo, esto no es práctico para datos sensibles.\n",
    "\n",
    "2. Enmascaramiento con Funciones Personalizadas:\n",
    "\n",
    "- En este caso, se utiliza una función personalizada (custom_masking_function) para enmascarar los valores de la columna sensible. Aunque el ejemplo usa una función simple de sustitución de caracteres, puedes personalizar la lógica de enmascaramiento según tus necesidades.\n",
    "\n",
    "- **La recuperación** en este caso depende de la lógica específica de enmascaramiento que hayas implementado. Si la función de enmascaramiento es reversible, podrías aplicar la función inversa para recuperar los datos originales. Sin embargo, muchas técnicas de enmascaramiento buscan ser irreversibles para mejorar la seguridad.\n",
    "\n",
    "3. Enmascaramiento con Anonimización de Datos:\n",
    "\n",
    "- Aquí, la columna sensible se anonimiza al reemplazar los valores con identificadores aleatorios generados mediante la función rand(). La anonimización implica la sustitución de datos originales con datos no identificables o pseudónimos.\n",
    "\n",
    "- La anonimización implica reemplazar datos con identificadores aleatorios o pseudónimos. **La recuperación** implica mantener una tabla de mapeo que asocia los identificadores aleatorios con los datos originales. La recuperación se realiza mediante la consulta de esta tabla de mapeo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62def5",
   "metadata": {},
   "source": [
    "### Enmascaramiento de Columnas con Funciones de Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc64a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2\n",
    "\n",
    "# Inicializa una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DataMaskingExample\").getOrCreate()\n",
    "\n",
    "# Carga los datos desde una fuente (por ejemplo, CSV)\n",
    "df = spark.read.csv(\"path/to/your/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Enmascara una columna usando SHA-256\n",
    "df_masked = df.withColumn(\"sensitive_column_masked\", sha2(\"sensitive_column\", 256))\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "df_masked.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4ff42",
   "metadata": {},
   "source": [
    "### Enmascaramiento con Funciones Personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Función de enmascaramiento personalizada (por ejemplo, sustitución de caracteres)\n",
    "def custom_masking_function(value):\n",
    "    # Implementa tu lógica de enmascaramiento aquí\n",
    "    return \"*****\"\n",
    "\n",
    "# Registra la función como una UDF (User Defined Function)\n",
    "masking_udf = udf(custom_masking_function, StringType())\n",
    "\n",
    "# Inicializa una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"CustomMaskingExample\").getOrCreate()\n",
    "\n",
    "# Carga los datos desde una fuente\n",
    "df = spark.read.csv(\"path/to/your/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Enmascara una columna usando la función personalizada\n",
    "df_masked = df.withColumn(\"sensitive_column_masked\", masking_udf(\"sensitive_column\"))\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "df_masked.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f301a67",
   "metadata": {},
   "source": [
    "### Enmascaramiento con Anonimización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "# Inicializa una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DataAnonymizationExample\").getOrCreate()\n",
    "\n",
    "# Carga los datos desde una fuente\n",
    "df = spark.read.csv(\"path/to/your/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Anonimiza una columna (por ejemplo, reemplaza valores con identificadores aleatorios)\n",
    "df_anonymized = df.withColumn(\"sensitive_column_anonymized\", (rand() * 1000).cast(\"int\"))\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "df_anonymized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc6f09",
   "metadata": {},
   "source": [
    "### Otro ejemplo con withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0d9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 10:44:41 WARN Utils: Your hostname, CompuDell01 resolves to a loopback address: 127.0.1.1; using 192.168.29.145 instead (on interface eth0)\n",
      "23/11/18 10:44:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/18 10:44:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Nombre|Correo_enmascarado|\n",
      "+------+------------------+\n",
      "|  Juan|  juan@example.com|\n",
      "| María| maria@example.com|\n",
      "| Pedro| pedro@example.com|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"ejemplo\").getOrCreate()\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = [(\"Juan\", \"juan@example.com\"),\n",
    "        (\"María\", \"maria@example.com\"),\n",
    "        (\"Pedro\", \"pedro@example.com\")]\n",
    "\n",
    "columns = [\"Nombre\", \"Correo\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Enmascarar el nombre de la columna de correo utilizando alias\n",
    "df_enmascarado = df.withColumn(\"Correo_enmascarado\", col(\"Correo\").alias(\"Correo\"))\n",
    "\n",
    "# Seleccionar solo las columnas deseadas\n",
    "df_enmascarado = df_enmascarado.select(\"Nombre\", \"Correo_enmascarado\")\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df_enmascarado.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748ba38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac527af",
   "metadata": {},
   "source": [
    "### <a name=\"mark_36\"></a> SHA-256 (Secure Hash Algorithm 256-bit).\n",
    "### [Index](#index) \n",
    "\n",
    "El algoritmo de hash SHA-256 (Secure Hash Algorithm 256-bit) es una función criptográfica de hash que produce un valor hash de 256 bits, generalmente expresado como una cadena hexadecimal de 64 caracteres. Este algoritmo pertenece a la familia de funciones hash SHA-2 y es ampliamente utilizado en aplicaciones de seguridad y criptografía.\n",
    "\n",
    "En el contexto de Spark o cualquier otro entorno de procesamiento de datos, SHA-256 a menudo se utiliza para la generación de resúmenes hash de datos sensibles o identificadores. Algunos casos de uso típicos incluyen:\n",
    "\n",
    "1. **Enmascaramiento de Datos Sensibles:**\n",
    "   - Se puede aplicar SHA-256 para generar hashes de valores sensibles como direcciones de correo electrónico, números de tarjetas de crédito, etc.\n",
    "   - Los hashes resultantes se almacenan en lugar de los datos originales, lo que proporciona una capa adicional de seguridad.\n",
    "\n",
    "2. **Detección de Cambios en Datos:**\n",
    "   - SHA-256 se utiliza para generar huellas dactilares (hashes) de conjuntos de datos.\n",
    "   - Si los datos cambian, los nuevos hashes serán diferentes, lo que permite detectar cambios de manera eficiente.\n",
    "\n",
    "3. **Integridad de Datos:**\n",
    "   - Para verificar la integridad de los datos almacenados, especialmente en sistemas distribuidos como Spark, donde los datos pueden estar distribuidos en clústeres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61cda02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------+\n",
      "|Name |Name_Hash                                                       |\n",
      "+-----+----------------------------------------------------------------+\n",
      "|John |a8cfcd74832004951b4408cdb0a5dbcd8c7e52d43f7fe244bf720582e05241da|\n",
      "|Doe  |fd53ef835b15485572a6e82cf470dcb41fd218ae5751ab7531c956a2a6bcd3c7|\n",
      "|Alice|3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043|\n",
      "+-----+----------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'En este ejemplo, se crea un nuevo DataFrame (`df_hashed`) que contiene una columna adicional \\n(\"Name_Hash\") que almacena los valores hash SHA-256 de la columna original \"Name\". \\nEste proceso ayuda a preservar la privacidad y seguridad de los datos originales.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Aquí hay un ejemplo simple en PySpark que utiliza SHA-256 para generar un hash de una \n",
    "columna en un DataFrame'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"SHA256Example\").getOrCreate()\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = [(\"John\",), (\"Doe\",), (\"Alice\",)]\n",
    "columns = [\"Name\"]\n",
    "schema = [StringType()]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Aplicar SHA-256 a la columna 'Name'\n",
    "df_hashed = df.withColumn(\"Name_Hash\", sha2(df[\"Name\"], 256))\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df_hashed.show(truncate=False)\n",
    "\n",
    "\n",
    "'''En este ejemplo, se crea un nuevo DataFrame (`df_hashed`) que contiene una columna adicional \n",
    "(\"Name_Hash\") que almacena los valores hash SHA-256 de la columna original \"Name\". \n",
    "Este proceso ayuda a preservar la privacidad y seguridad de los datos originales.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f3a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8217b2d",
   "metadata": {},
   "source": [
    "### <a name=\"solucion_python_worker_versiones\"></a>solucion python worker dif. versiones\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "### Inconveniente_01 --> RuntimeError: Python in worker has different version 3.10 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
    "\n",
    "Solución:\n",
    "\n",
    "\n",
    "Para solucionar el siguiente error:\n",
    "\n",
    "RuntimeError: Python in worker has different version 3.10 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
    "\n",
    "Debes asegurarte de que la versión de Python que está utilizando el controlador de PySpark sea la misma que la versión de Python que están utilizando los trabajadores de PySpark. Para hacer esto, puedes configurar las variables de entorno PYSPARK_PYTHON y PYSPARK_DRIVER_PYTHON.\n",
    "\n",
    "- Puedes configurar estas variables de entorno en el archivo ~/.bashrc o en el archivo spark-env.sh.\n",
    "\n",
    "- Para configurar las variables de entorno en el archivo ~/.bashrc, agrega las siguientes líneas al final del archivo:\n",
    "\n",
    "- export PYSPARK_PYTHON=/ruta/a/python3.11\n",
    "\n",
    "- export PYSPARK_DRIVER_PYTHON=/ruta/a/python3.11\n",
    "\n",
    "- Para configurar las variables de entorno en el archivo spark-env.sh, agrega las siguientes líneas al final del archivo:\n",
    "\n",
    "- PYSPARK_PYTHON=/ruta/a/python3.11\n",
    "\n",
    "- PYSPARK_DRIVER_PYTHON=/ruta/a/python3.11\n",
    "\n",
    "Una vez que hayas configurado las variables de entorno, guarda los cambios y reinicia tu sesión de terminal.\n",
    "\n",
    "- Mi elección fue ir por \".bashrc\"\n",
    "\n",
    "![](img_17.png)\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "Con el comando \"nano .bashrc\"\n",
    "\n",
    "![](img_18.png)\n",
    "\n",
    "ctr+x para salir y \"yes\" para salir y guardar los cambios.\n",
    "**-------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel_01",
   "language": "python",
   "name": "data_trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
